{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T01:20:13.716154Z",
     "start_time": "2019-01-30T01:20:09.114269Z"
    }
   },
   "outputs": [],
   "source": [
    "#REFERENCE:https://tf.wiki/zh/models.html#mlp\n",
    "#eager_execution\n",
    "#半残废\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "imageBigDataORL = np.zeros((400, 112*92))\n",
    "YBigData = np.zeros(400)\n",
    "\n",
    "def data_prep_ORL():\n",
    "    for people in range(1,41):\n",
    "        for face in range(1,11):\n",
    "            path = './ORL/s%d_%d.bmp' % (people, face)\n",
    "            oriImage = PIL.Image.open(path)\n",
    "            imageArray = np.array(oriImage)\n",
    "#             print(imageArray)\n",
    "            imageVec = np.reshape(imageArray, imageArray.shape[0] * imageArray.shape[1])\n",
    "            # print(imageArray.shape)\n",
    "            # print(imageBigDataORL.shape)\n",
    "#             print((people - 1) * 10 + face)\n",
    "            imageBigDataORL[(people - 1) * 10 + face - 1] = imageVec\n",
    "            YBigData[(people - 1) * 10 + face - 1] = people - 1\n",
    "#             print(imageBigDataORL)\n",
    "\n",
    "def get_batch(size):\n",
    "    index = np.random.randint(0, np.shape(X_train)[0], size)\n",
    "    return X_train[index, :], Y_train[index]\n",
    "\n",
    "class simplePerceptron(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=400, activation=tf.nn.relu)\n",
    "        self.dense3 = tf.keras.layers.Dense(units=400, activation=tf.nn.relu)\n",
    "        self.dense4 = tf.keras.layers.Dense(units=400, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=40)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dense4(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        logits = self(inputs)\n",
    "        return tf.argmax(logits, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T01:20:52.416479Z",
     "start_time": "2019-01-30T01:20:13.718818Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 195.034531\n",
      "batch 1: loss 170.367767\n",
      "batch 2: loss 146.436218\n",
      "batch 3: loss 119.513611\n",
      "batch 4: loss 100.922462\n",
      "batch 5: loss 94.663635\n",
      "batch 6: loss 87.458809\n",
      "batch 7: loss 83.023170\n",
      "batch 8: loss 76.461075\n",
      "batch 9: loss 80.399567\n",
      "batch 10: loss 60.934902\n",
      "batch 11: loss 52.898991\n",
      "batch 12: loss 58.109779\n",
      "batch 13: loss 47.753761\n",
      "batch 14: loss 56.228310\n",
      "batch 15: loss 46.904633\n",
      "batch 16: loss 42.369995\n",
      "batch 17: loss 50.353065\n",
      "batch 18: loss 38.939335\n",
      "batch 19: loss 35.669777\n",
      "batch 20: loss 37.974991\n",
      "batch 21: loss 39.209667\n",
      "batch 22: loss 32.333984\n",
      "batch 23: loss 37.765560\n",
      "batch 24: loss 27.858032\n",
      "batch 25: loss 24.864870\n",
      "batch 26: loss 30.229370\n",
      "batch 27: loss 37.888847\n",
      "batch 28: loss 23.120222\n",
      "batch 29: loss 27.429638\n",
      "batch 30: loss 28.191187\n",
      "batch 31: loss 26.986597\n",
      "batch 32: loss 26.167997\n",
      "batch 33: loss 21.639381\n",
      "batch 34: loss 29.251110\n",
      "batch 35: loss 21.693134\n",
      "batch 36: loss 25.066494\n",
      "batch 37: loss 18.157013\n",
      "batch 38: loss 23.030054\n",
      "batch 39: loss 20.299402\n",
      "batch 40: loss 22.619177\n",
      "batch 41: loss 14.777832\n",
      "batch 42: loss 17.609966\n",
      "batch 43: loss 17.974827\n",
      "batch 44: loss 20.584139\n",
      "batch 45: loss 20.988972\n",
      "batch 46: loss 19.223034\n",
      "batch 47: loss 14.733768\n",
      "batch 48: loss 15.882025\n",
      "batch 49: loss 16.712526\n",
      "batch 50: loss 11.674816\n",
      "batch 51: loss 16.066652\n",
      "batch 52: loss 14.272919\n",
      "batch 53: loss 14.059113\n",
      "batch 54: loss 11.285935\n",
      "batch 55: loss 10.490973\n",
      "batch 56: loss 12.108634\n",
      "batch 57: loss 11.900849\n",
      "batch 58: loss 13.798566\n",
      "batch 59: loss 13.401590\n",
      "batch 60: loss 12.089072\n",
      "batch 61: loss 11.766494\n",
      "batch 62: loss 10.058254\n",
      "batch 63: loss 13.123408\n",
      "batch 64: loss 11.206964\n",
      "batch 65: loss 10.342250\n",
      "batch 66: loss 10.015497\n",
      "batch 67: loss 11.246451\n",
      "batch 68: loss 12.796191\n",
      "batch 69: loss 8.362906\n",
      "batch 70: loss 10.424963\n",
      "batch 71: loss 8.375576\n",
      "batch 72: loss 7.505558\n",
      "batch 73: loss 10.690077\n",
      "batch 74: loss 8.577066\n",
      "batch 75: loss 9.819701\n",
      "batch 76: loss 9.113732\n",
      "batch 77: loss 8.968227\n",
      "batch 78: loss 7.139695\n",
      "batch 79: loss 5.531610\n",
      "batch 80: loss 8.183216\n",
      "batch 81: loss 7.145903\n",
      "batch 82: loss 5.435497\n",
      "batch 83: loss 7.168661\n",
      "batch 84: loss 5.492665\n",
      "batch 85: loss 5.607889\n",
      "batch 86: loss 6.062834\n",
      "batch 87: loss 4.110325\n",
      "batch 88: loss 4.453923\n",
      "batch 89: loss 8.755931\n",
      "batch 90: loss 5.004912\n",
      "batch 91: loss 3.618732\n",
      "batch 92: loss 3.873954\n",
      "batch 93: loss 5.597702\n",
      "batch 94: loss 5.723373\n",
      "batch 95: loss 6.233778\n",
      "batch 96: loss 4.681413\n",
      "batch 97: loss 6.332870\n",
      "batch 98: loss 5.376645\n",
      "batch 99: loss 3.775918\n",
      "batch 100: loss 4.332520\n",
      "batch 101: loss 3.645369\n",
      "batch 102: loss 5.345972\n",
      "batch 103: loss 3.467180\n",
      "batch 104: loss 4.400538\n",
      "batch 105: loss 2.455823\n",
      "batch 106: loss 4.197520\n",
      "batch 107: loss 3.654251\n",
      "batch 108: loss 2.747854\n",
      "batch 109: loss 2.476343\n",
      "batch 110: loss 2.863957\n",
      "batch 111: loss 3.377081\n",
      "batch 112: loss 2.274536\n",
      "batch 113: loss 1.117252\n",
      "batch 114: loss 2.824722\n",
      "batch 115: loss 2.619097\n",
      "batch 116: loss 2.703976\n",
      "batch 117: loss 2.442760\n",
      "batch 118: loss 3.322034\n",
      "batch 119: loss 2.185337\n",
      "batch 120: loss 1.600840\n",
      "batch 121: loss 2.100119\n",
      "batch 122: loss 1.418393\n",
      "batch 123: loss 1.598825\n",
      "batch 124: loss 1.473158\n",
      "batch 125: loss 2.107005\n",
      "batch 126: loss 1.523186\n",
      "batch 127: loss 1.366102\n",
      "batch 128: loss 1.856531\n",
      "batch 129: loss 3.906322\n",
      "batch 130: loss 1.469952\n",
      "batch 131: loss 1.318372\n",
      "batch 132: loss 1.399662\n",
      "batch 133: loss 1.314483\n",
      "batch 134: loss 2.194448\n",
      "batch 135: loss 2.476665\n",
      "batch 136: loss 1.970653\n",
      "batch 137: loss 2.104107\n",
      "batch 138: loss 1.221694\n",
      "batch 139: loss 1.830685\n",
      "batch 140: loss 1.206543\n",
      "batch 141: loss 1.401314\n",
      "batch 142: loss 0.647078\n",
      "batch 143: loss 0.868766\n",
      "batch 144: loss 1.655692\n",
      "batch 145: loss 2.574771\n",
      "batch 146: loss 2.415751\n",
      "batch 147: loss 1.572789\n",
      "batch 148: loss 1.992363\n",
      "batch 149: loss 0.437692\n",
      "batch 150: loss 0.314453\n",
      "batch 151: loss 0.904204\n",
      "batch 152: loss 0.696884\n",
      "batch 153: loss 3.221246\n",
      "batch 154: loss 1.005291\n",
      "batch 155: loss 0.609253\n",
      "batch 156: loss 1.247032\n",
      "batch 157: loss 1.218707\n",
      "batch 158: loss 0.857256\n",
      "batch 159: loss 0.895208\n",
      "batch 160: loss 0.618408\n",
      "batch 161: loss 0.478194\n",
      "batch 162: loss 0.960146\n",
      "batch 163: loss 1.029498\n",
      "batch 164: loss 0.504722\n",
      "batch 165: loss 0.674880\n",
      "batch 166: loss 0.516783\n",
      "batch 167: loss 0.526191\n",
      "batch 168: loss 0.700808\n",
      "batch 169: loss 0.319237\n",
      "batch 170: loss 0.742818\n",
      "batch 171: loss 0.524730\n",
      "batch 172: loss 0.216230\n",
      "batch 173: loss 0.300424\n",
      "batch 174: loss 0.311125\n",
      "batch 175: loss 0.781845\n",
      "batch 176: loss 0.440019\n",
      "batch 177: loss 0.455004\n",
      "batch 178: loss 0.498644\n",
      "batch 179: loss 0.499033\n",
      "batch 180: loss 1.364377\n",
      "batch 181: loss 0.977347\n",
      "batch 182: loss 0.432958\n",
      "batch 183: loss 0.763028\n",
      "batch 184: loss 0.453988\n",
      "batch 185: loss 0.361903\n",
      "batch 186: loss 0.473299\n",
      "batch 187: loss 0.418992\n",
      "batch 188: loss 0.526657\n",
      "batch 189: loss 0.192498\n",
      "batch 190: loss 0.403757\n",
      "batch 191: loss 0.266885\n",
      "batch 192: loss 0.189216\n",
      "batch 193: loss 0.057863\n",
      "batch 194: loss 0.154204\n",
      "batch 195: loss 0.337204\n",
      "batch 196: loss 0.324720\n",
      "batch 197: loss 0.095576\n",
      "batch 198: loss 0.215642\n",
      "batch 199: loss 0.116864\n",
      "batch 200: loss 0.511526\n",
      "batch 201: loss 0.523644\n",
      "batch 202: loss 0.256907\n",
      "batch 203: loss 0.297575\n",
      "batch 204: loss 0.060976\n",
      "batch 205: loss 0.654365\n",
      "batch 206: loss 0.148168\n",
      "batch 207: loss 0.442264\n",
      "batch 208: loss 0.360780\n",
      "batch 209: loss 0.131739\n",
      "batch 210: loss 0.398172\n",
      "batch 211: loss 0.254828\n",
      "batch 212: loss 0.283078\n",
      "batch 213: loss 0.093254\n",
      "batch 214: loss 0.289335\n",
      "batch 215: loss 0.194122\n",
      "batch 216: loss 0.103713\n",
      "batch 217: loss 0.120019\n",
      "batch 218: loss 0.372153\n",
      "batch 219: loss 0.200326\n",
      "batch 220: loss 0.170476\n",
      "batch 221: loss 0.028471\n",
      "batch 222: loss 0.015448\n",
      "batch 223: loss 0.039491\n",
      "batch 224: loss 0.025092\n",
      "batch 225: loss 0.194721\n",
      "batch 226: loss 0.032668\n",
      "batch 227: loss 0.019861\n",
      "batch 228: loss 0.140376\n",
      "batch 229: loss 0.077108\n",
      "batch 230: loss 0.098155\n",
      "batch 231: loss 0.050499\n",
      "batch 232: loss 0.043075\n",
      "batch 233: loss 0.018810\n",
      "batch 234: loss 0.075394\n",
      "batch 235: loss 0.097585\n",
      "batch 236: loss 0.064430\n",
      "batch 237: loss 0.151591\n",
      "batch 238: loss 0.036202\n",
      "batch 239: loss 0.013370\n",
      "batch 240: loss 0.015472\n",
      "batch 241: loss 0.023227\n",
      "batch 242: loss 0.014912\n",
      "batch 243: loss 0.115044\n",
      "batch 244: loss 0.090100\n",
      "batch 245: loss 0.025248\n",
      "batch 246: loss 0.021314\n",
      "batch 247: loss 0.021915\n",
      "batch 248: loss 0.017176\n",
      "batch 249: loss 0.080140\n",
      "batch 250: loss 0.008608\n",
      "batch 251: loss 0.021235\n",
      "batch 252: loss 0.018425\n",
      "batch 253: loss 0.029323\n",
      "batch 254: loss 0.024655\n",
      "batch 255: loss 0.049060\n",
      "batch 256: loss 0.012374\n",
      "batch 257: loss 0.005007\n",
      "batch 258: loss 0.007546\n",
      "batch 259: loss 0.021307\n",
      "batch 260: loss 0.006832\n",
      "batch 261: loss 0.013738\n",
      "batch 262: loss 0.004338\n",
      "batch 263: loss 0.010787\n",
      "batch 264: loss 0.007119\n",
      "batch 265: loss 0.005307\n",
      "batch 266: loss 0.011862\n",
      "batch 267: loss 0.017192\n",
      "batch 268: loss 0.048610\n",
      "batch 269: loss 0.008035\n",
      "batch 270: loss 0.011070\n",
      "batch 271: loss 0.010247\n",
      "batch 272: loss 0.008791\n",
      "batch 273: loss 0.007148\n",
      "batch 274: loss 0.004952\n",
      "batch 275: loss 0.011723\n",
      "batch 276: loss 0.004587\n",
      "batch 277: loss 0.006374\n",
      "batch 278: loss 0.011379\n",
      "batch 279: loss 0.005149\n",
      "batch 280: loss 0.007605\n",
      "batch 281: loss 0.009106\n",
      "batch 282: loss 0.002010\n",
      "batch 283: loss 0.004693\n",
      "batch 284: loss 0.005045\n",
      "batch 285: loss 0.010421\n",
      "batch 286: loss 0.007821\n",
      "batch 287: loss 0.007185\n",
      "batch 288: loss 0.002996\n",
      "batch 289: loss 0.001808\n",
      "batch 290: loss 0.002905\n",
      "batch 291: loss 0.005033\n",
      "batch 292: loss 0.006544\n",
      "batch 293: loss 0.008471\n",
      "batch 294: loss 0.007308\n",
      "batch 295: loss 0.003174\n",
      "batch 296: loss 0.005597\n",
      "batch 297: loss 0.004124\n",
      "batch 298: loss 0.006223\n",
      "batch 299: loss 0.004350\n",
      "batch 300: loss 0.003229\n",
      "batch 301: loss 0.005508\n",
      "batch 302: loss 0.004977\n",
      "batch 303: loss 0.003818\n",
      "batch 304: loss 0.007069\n",
      "batch 305: loss 0.003452\n",
      "batch 306: loss 0.004097\n",
      "batch 307: loss 0.003406\n",
      "batch 308: loss 0.003131\n",
      "batch 309: loss 0.003200\n",
      "batch 310: loss 0.003696\n",
      "batch 311: loss 0.002587\n",
      "batch 312: loss 0.004160\n",
      "batch 313: loss 0.005030\n",
      "batch 314: loss 0.001313\n",
      "batch 315: loss 0.003896\n",
      "batch 316: loss 0.002807\n",
      "batch 317: loss 0.001715\n",
      "batch 318: loss 0.004840\n",
      "batch 319: loss 0.001218\n",
      "batch 320: loss 0.002518\n",
      "batch 321: loss 0.002386\n",
      "batch 322: loss 0.002243\n",
      "batch 323: loss 0.002132\n",
      "batch 324: loss 0.002668\n",
      "batch 325: loss 0.002519\n",
      "batch 326: loss 0.002603\n",
      "batch 327: loss 0.001615\n",
      "batch 328: loss 0.001648\n",
      "batch 329: loss 0.002267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 330: loss 0.002856\n",
      "batch 331: loss 0.002630\n",
      "batch 332: loss 0.001973\n",
      "batch 333: loss 0.003284\n",
      "batch 334: loss 0.004334\n",
      "batch 335: loss 0.003469\n",
      "batch 336: loss 0.002034\n",
      "batch 337: loss 0.002812\n",
      "batch 338: loss 0.002721\n",
      "batch 339: loss 0.001483\n",
      "batch 340: loss 0.001929\n",
      "batch 341: loss 0.003074\n",
      "batch 342: loss 0.002270\n",
      "batch 343: loss 0.002365\n",
      "batch 344: loss 0.002188\n",
      "batch 345: loss 0.003154\n",
      "batch 346: loss 0.001690\n",
      "batch 347: loss 0.003687\n",
      "batch 348: loss 0.002091\n",
      "batch 349: loss 0.003426\n",
      "batch 350: loss 0.003194\n",
      "batch 351: loss 0.002476\n",
      "batch 352: loss 0.001897\n",
      "batch 353: loss 0.002268\n",
      "batch 354: loss 0.001902\n",
      "batch 355: loss 0.002343\n",
      "batch 356: loss 0.001764\n",
      "batch 357: loss 0.001764\n",
      "batch 358: loss 0.002249\n",
      "batch 359: loss 0.002446\n",
      "batch 360: loss 0.002919\n",
      "batch 361: loss 0.003342\n",
      "batch 362: loss 0.002851\n",
      "batch 363: loss 0.002056\n",
      "batch 364: loss 0.002614\n",
      "batch 365: loss 0.001182\n",
      "batch 366: loss 0.002108\n",
      "batch 367: loss 0.002183\n",
      "batch 368: loss 0.002242\n",
      "batch 369: loss 0.002058\n",
      "batch 370: loss 0.002335\n",
      "batch 371: loss 0.002455\n",
      "batch 372: loss 0.001728\n",
      "batch 373: loss 0.002570\n",
      "batch 374: loss 0.001420\n",
      "batch 375: loss 0.001611\n",
      "batch 376: loss 0.002311\n",
      "batch 377: loss 0.001531\n",
      "batch 378: loss 0.001855\n",
      "batch 379: loss 0.001126\n",
      "batch 380: loss 0.001542\n",
      "batch 381: loss 0.001750\n",
      "batch 382: loss 0.002767\n",
      "batch 383: loss 0.001450\n",
      "batch 384: loss 0.002271\n",
      "batch 385: loss 0.002393\n",
      "batch 386: loss 0.001938\n",
      "batch 387: loss 0.002000\n",
      "batch 388: loss 0.000917\n",
      "batch 389: loss 0.001347\n",
      "batch 390: loss 0.002245\n",
      "batch 391: loss 0.002704\n",
      "batch 392: loss 0.002856\n",
      "batch 393: loss 0.002137\n",
      "batch 394: loss 0.001793\n",
      "batch 395: loss 0.002829\n",
      "batch 396: loss 0.002823\n",
      "batch 397: loss 0.001975\n",
      "batch 398: loss 0.001946\n",
      "batch 399: loss 0.001905\n",
      "batch 400: loss 0.002208\n",
      "batch 401: loss 0.001815\n",
      "batch 402: loss 0.002127\n",
      "batch 403: loss 0.001213\n",
      "batch 404: loss 0.001472\n",
      "batch 405: loss 0.001556\n",
      "batch 406: loss 0.002052\n",
      "batch 407: loss 0.001683\n",
      "batch 408: loss 0.002260\n",
      "batch 409: loss 0.002198\n",
      "batch 410: loss 0.001647\n",
      "batch 411: loss 0.002299\n",
      "batch 412: loss 0.002401\n",
      "batch 413: loss 0.001301\n",
      "batch 414: loss 0.001779\n",
      "batch 415: loss 0.001121\n",
      "batch 416: loss 0.001613\n",
      "batch 417: loss 0.001643\n",
      "batch 418: loss 0.002042\n",
      "batch 419: loss 0.001405\n",
      "batch 420: loss 0.002194\n",
      "batch 421: loss 0.001249\n",
      "batch 422: loss 0.001800\n",
      "batch 423: loss 0.001854\n",
      "batch 424: loss 0.000890\n",
      "batch 425: loss 0.001740\n",
      "batch 426: loss 0.001886\n",
      "batch 427: loss 0.001994\n",
      "batch 428: loss 0.001689\n",
      "batch 429: loss 0.002004\n",
      "batch 430: loss 0.001261\n",
      "batch 431: loss 0.001568\n",
      "batch 432: loss 0.001474\n",
      "batch 433: loss 0.001406\n",
      "batch 434: loss 0.001698\n",
      "batch 435: loss 0.001354\n",
      "batch 436: loss 0.002302\n",
      "batch 437: loss 0.001402\n",
      "batch 438: loss 0.001185\n",
      "batch 439: loss 0.001631\n",
      "batch 440: loss 0.002292\n",
      "batch 441: loss 0.001979\n",
      "batch 442: loss 0.001545\n",
      "batch 443: loss 0.001672\n",
      "batch 444: loss 0.001286\n",
      "batch 445: loss 0.002213\n",
      "batch 446: loss 0.001941\n",
      "batch 447: loss 0.001072\n",
      "batch 448: loss 0.000994\n",
      "batch 449: loss 0.002353\n",
      "batch 450: loss 0.002614\n",
      "batch 451: loss 0.002231\n",
      "batch 452: loss 0.001151\n",
      "batch 453: loss 0.001487\n",
      "batch 454: loss 0.001525\n",
      "batch 455: loss 0.001725\n",
      "batch 456: loss 0.002025\n",
      "batch 457: loss 0.001627\n",
      "batch 458: loss 0.002972\n",
      "batch 459: loss 0.000889\n",
      "batch 460: loss 0.002078\n",
      "batch 461: loss 0.001541\n",
      "batch 462: loss 0.001728\n",
      "batch 463: loss 0.001403\n",
      "batch 464: loss 0.001573\n",
      "batch 465: loss 0.001231\n",
      "batch 466: loss 0.001173\n",
      "batch 467: loss 0.001389\n",
      "batch 468: loss 0.001705\n",
      "batch 469: loss 0.001872\n",
      "batch 470: loss 0.001208\n",
      "batch 471: loss 0.000798\n",
      "batch 472: loss 0.001966\n",
      "batch 473: loss 0.001164\n",
      "batch 474: loss 0.001490\n",
      "batch 475: loss 0.001379\n",
      "batch 476: loss 0.001548\n",
      "batch 477: loss 0.001610\n",
      "batch 478: loss 0.001773\n",
      "batch 479: loss 0.001047\n",
      "batch 480: loss 0.002126\n",
      "batch 481: loss 0.002773\n",
      "batch 482: loss 0.001253\n",
      "batch 483: loss 0.001662\n",
      "batch 484: loss 0.001712\n",
      "batch 485: loss 0.000933\n",
      "batch 486: loss 0.000883\n",
      "batch 487: loss 0.001218\n",
      "batch 488: loss 0.001227\n",
      "batch 489: loss 0.001554\n",
      "batch 490: loss 0.001203\n",
      "batch 491: loss 0.001344\n",
      "batch 492: loss 0.001135\n",
      "batch 493: loss 0.000985\n",
      "batch 494: loss 0.001592\n",
      "batch 495: loss 0.001585\n",
      "batch 496: loss 0.001263\n",
      "batch 497: loss 0.001653\n",
      "batch 498: loss 0.001275\n",
      "batch 499: loss 0.001035\n",
      "batch 500: loss 0.000970\n",
      "batch 501: loss 0.001325\n",
      "batch 502: loss 0.001556\n",
      "batch 503: loss 0.000997\n",
      "batch 504: loss 0.001552\n",
      "batch 505: loss 0.001273\n",
      "batch 506: loss 0.001515\n",
      "batch 507: loss 0.001076\n",
      "batch 508: loss 0.001594\n",
      "batch 509: loss 0.001938\n",
      "batch 510: loss 0.001377\n",
      "batch 511: loss 0.000874\n",
      "batch 512: loss 0.000978\n",
      "batch 513: loss 0.001020\n",
      "batch 514: loss 0.001180\n",
      "batch 515: loss 0.001454\n",
      "batch 516: loss 0.001498\n",
      "batch 517: loss 0.001553\n",
      "batch 518: loss 0.001143\n",
      "batch 519: loss 0.002028\n",
      "batch 520: loss 0.001486\n",
      "batch 521: loss 0.001401\n",
      "batch 522: loss 0.001420\n",
      "batch 523: loss 0.000981\n",
      "batch 524: loss 0.000961\n",
      "batch 525: loss 0.002426\n",
      "batch 526: loss 0.001318\n",
      "batch 527: loss 0.001491\n",
      "batch 528: loss 0.001700\n",
      "batch 529: loss 0.001450\n",
      "batch 530: loss 0.001517\n",
      "batch 531: loss 0.001035\n",
      "batch 532: loss 0.001544\n",
      "batch 533: loss 0.001104\n",
      "batch 534: loss 0.001069\n",
      "batch 535: loss 0.001435\n",
      "batch 536: loss 0.001232\n",
      "batch 537: loss 0.001260\n",
      "batch 538: loss 0.001202\n",
      "batch 539: loss 0.001376\n",
      "batch 540: loss 0.001112\n",
      "batch 541: loss 0.001804\n",
      "batch 542: loss 0.001922\n",
      "batch 543: loss 0.001208\n",
      "batch 544: loss 0.001395\n",
      "batch 545: loss 0.000921\n",
      "batch 546: loss 0.000486\n",
      "batch 547: loss 0.001793\n",
      "batch 548: loss 0.001151\n",
      "batch 549: loss 0.001564\n",
      "batch 550: loss 0.001476\n",
      "batch 551: loss 0.001206\n",
      "batch 552: loss 0.001271\n",
      "batch 553: loss 0.000930\n",
      "batch 554: loss 0.001562\n",
      "batch 555: loss 0.001330\n",
      "batch 556: loss 0.002186\n",
      "batch 557: loss 0.001621\n",
      "batch 558: loss 0.001358\n",
      "batch 559: loss 0.001576\n",
      "batch 560: loss 0.001268\n",
      "batch 561: loss 0.000743\n",
      "batch 562: loss 0.001282\n",
      "batch 563: loss 0.001477\n",
      "batch 564: loss 0.001362\n",
      "batch 565: loss 0.001057\n",
      "batch 566: loss 0.001406\n",
      "batch 567: loss 0.001124\n",
      "batch 568: loss 0.001188\n",
      "batch 569: loss 0.001493\n",
      "batch 570: loss 0.001173\n",
      "batch 571: loss 0.001056\n",
      "batch 572: loss 0.001431\n",
      "batch 573: loss 0.001372\n",
      "batch 574: loss 0.001592\n",
      "batch 575: loss 0.001331\n",
      "batch 576: loss 0.000862\n",
      "batch 577: loss 0.001100\n",
      "batch 578: loss 0.000945\n",
      "batch 579: loss 0.000791\n",
      "batch 580: loss 0.001357\n",
      "batch 581: loss 0.001327\n",
      "batch 582: loss 0.001278\n",
      "batch 583: loss 0.000981\n",
      "batch 584: loss 0.001417\n",
      "batch 585: loss 0.001446\n",
      "batch 586: loss 0.001547\n",
      "batch 587: loss 0.001048\n",
      "batch 588: loss 0.001166\n",
      "batch 589: loss 0.001285\n",
      "batch 590: loss 0.001588\n",
      "batch 591: loss 0.000723\n",
      "batch 592: loss 0.001106\n",
      "batch 593: loss 0.001300\n",
      "batch 594: loss 0.001281\n",
      "batch 595: loss 0.001132\n",
      "batch 596: loss 0.001046\n",
      "batch 597: loss 0.001246\n",
      "batch 598: loss 0.001288\n",
      "batch 599: loss 0.001501\n",
      "batch 600: loss 0.001391\n",
      "batch 601: loss 0.001061\n",
      "batch 602: loss 0.001381\n",
      "batch 603: loss 0.001072\n",
      "batch 604: loss 0.001572\n",
      "batch 605: loss 0.001270\n",
      "batch 606: loss 0.001257\n",
      "batch 607: loss 0.001541\n",
      "batch 608: loss 0.001385\n",
      "batch 609: loss 0.001355\n",
      "batch 610: loss 0.001082\n",
      "batch 611: loss 0.001120\n",
      "batch 612: loss 0.001720\n",
      "batch 613: loss 0.001310\n",
      "batch 614: loss 0.000677\n",
      "batch 615: loss 0.000575\n",
      "batch 616: loss 0.001090\n",
      "batch 617: loss 0.001359\n",
      "batch 618: loss 0.001386\n",
      "batch 619: loss 0.001061\n",
      "batch 620: loss 0.001010\n",
      "batch 621: loss 0.000852\n",
      "batch 622: loss 0.000941\n",
      "batch 623: loss 0.001058\n",
      "batch 624: loss 0.000984\n",
      "batch 625: loss 0.001180\n",
      "batch 626: loss 0.001042\n",
      "batch 627: loss 0.001832\n",
      "batch 628: loss 0.001206\n",
      "batch 629: loss 0.001127\n",
      "batch 630: loss 0.001346\n",
      "batch 631: loss 0.000621\n",
      "batch 632: loss 0.001156\n",
      "batch 633: loss 0.001196\n",
      "batch 634: loss 0.000871\n",
      "batch 635: loss 0.001189\n",
      "batch 636: loss 0.001071\n",
      "batch 637: loss 0.000926\n",
      "batch 638: loss 0.001276\n",
      "batch 639: loss 0.001043\n",
      "batch 640: loss 0.000773\n",
      "batch 641: loss 0.001346\n",
      "batch 642: loss 0.001149\n",
      "batch 643: loss 0.001085\n",
      "batch 644: loss 0.001495\n",
      "batch 645: loss 0.001313\n",
      "batch 646: loss 0.001159\n",
      "batch 647: loss 0.000916\n",
      "batch 648: loss 0.001194\n",
      "batch 649: loss 0.001183\n",
      "batch 650: loss 0.000956\n",
      "batch 651: loss 0.000921\n",
      "batch 652: loss 0.000744\n",
      "batch 653: loss 0.000888\n",
      "batch 654: loss 0.001290\n",
      "batch 655: loss 0.000998\n",
      "batch 656: loss 0.001046\n",
      "batch 657: loss 0.001097\n",
      "batch 658: loss 0.000869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 659: loss 0.000996\n",
      "batch 660: loss 0.000924\n",
      "batch 661: loss 0.001380\n",
      "batch 662: loss 0.000886\n",
      "batch 663: loss 0.000928\n",
      "batch 664: loss 0.001558\n",
      "batch 665: loss 0.001690\n",
      "batch 666: loss 0.000873\n",
      "batch 667: loss 0.000786\n",
      "batch 668: loss 0.001076\n",
      "batch 669: loss 0.001383\n",
      "batch 670: loss 0.000546\n",
      "batch 671: loss 0.001165\n",
      "batch 672: loss 0.001142\n",
      "batch 673: loss 0.000916\n",
      "batch 674: loss 0.001066\n",
      "batch 675: loss 0.001271\n",
      "batch 676: loss 0.000757\n",
      "batch 677: loss 0.001143\n",
      "batch 678: loss 0.001119\n",
      "batch 679: loss 0.000878\n",
      "batch 680: loss 0.001126\n",
      "batch 681: loss 0.000725\n",
      "batch 682: loss 0.001165\n",
      "batch 683: loss 0.000786\n",
      "batch 684: loss 0.001160\n",
      "batch 685: loss 0.000706\n",
      "batch 686: loss 0.001165\n",
      "batch 687: loss 0.001256\n",
      "batch 688: loss 0.000959\n",
      "batch 689: loss 0.000929\n",
      "batch 690: loss 0.000925\n",
      "batch 691: loss 0.000964\n",
      "batch 692: loss 0.001124\n",
      "batch 693: loss 0.001111\n",
      "batch 694: loss 0.001289\n",
      "batch 695: loss 0.000920\n",
      "batch 696: loss 0.001431\n",
      "batch 697: loss 0.000982\n",
      "batch 698: loss 0.000428\n",
      "batch 699: loss 0.001439\n",
      "batch 700: loss 0.000810\n",
      "batch 701: loss 0.001313\n",
      "batch 702: loss 0.000994\n",
      "batch 703: loss 0.000985\n",
      "batch 704: loss 0.000867\n",
      "batch 705: loss 0.000760\n",
      "batch 706: loss 0.000718\n",
      "batch 707: loss 0.000948\n",
      "batch 708: loss 0.001127\n",
      "batch 709: loss 0.001217\n",
      "batch 710: loss 0.001191\n",
      "batch 711: loss 0.000907\n",
      "batch 712: loss 0.000971\n",
      "batch 713: loss 0.001640\n",
      "batch 714: loss 0.000954\n",
      "batch 715: loss 0.000810\n",
      "batch 716: loss 0.000937\n",
      "batch 717: loss 0.001131\n",
      "batch 718: loss 0.001017\n",
      "batch 719: loss 0.000579\n",
      "batch 720: loss 0.000831\n",
      "batch 721: loss 0.000699\n",
      "batch 722: loss 0.000787\n",
      "batch 723: loss 0.001125\n",
      "batch 724: loss 0.001291\n",
      "batch 725: loss 0.000894\n",
      "batch 726: loss 0.000936\n",
      "batch 727: loss 0.000926\n",
      "batch 728: loss 0.001042\n",
      "batch 729: loss 0.000934\n",
      "batch 730: loss 0.001302\n",
      "batch 731: loss 0.001047\n",
      "batch 732: loss 0.001193\n",
      "batch 733: loss 0.001084\n",
      "batch 734: loss 0.001092\n",
      "batch 735: loss 0.001050\n",
      "batch 736: loss 0.000897\n",
      "batch 737: loss 0.000859\n",
      "batch 738: loss 0.001312\n",
      "batch 739: loss 0.000621\n",
      "batch 740: loss 0.000941\n",
      "batch 741: loss 0.000970\n",
      "batch 742: loss 0.001143\n",
      "batch 743: loss 0.001142\n",
      "batch 744: loss 0.000402\n",
      "batch 745: loss 0.001099\n",
      "batch 746: loss 0.000879\n",
      "batch 747: loss 0.000844\n",
      "batch 748: loss 0.001076\n",
      "batch 749: loss 0.000949\n",
      "batch 750: loss 0.000999\n",
      "batch 751: loss 0.000699\n",
      "batch 752: loss 0.000839\n",
      "batch 753: loss 0.001234\n",
      "batch 754: loss 0.001248\n",
      "batch 755: loss 0.000721\n",
      "batch 756: loss 0.001057\n",
      "batch 757: loss 0.000699\n",
      "batch 758: loss 0.001303\n",
      "batch 759: loss 0.001227\n",
      "batch 760: loss 0.000974\n",
      "batch 761: loss 0.001133\n",
      "batch 762: loss 0.001044\n",
      "batch 763: loss 0.000793\n",
      "batch 764: loss 0.000805\n",
      "batch 765: loss 0.000825\n",
      "batch 766: loss 0.000766\n",
      "batch 767: loss 0.001431\n",
      "batch 768: loss 0.000756\n",
      "batch 769: loss 0.000840\n",
      "batch 770: loss 0.000866\n",
      "batch 771: loss 0.001007\n",
      "batch 772: loss 0.000816\n",
      "batch 773: loss 0.000773\n",
      "batch 774: loss 0.000876\n",
      "batch 775: loss 0.000885\n",
      "batch 776: loss 0.000778\n",
      "batch 777: loss 0.000966\n",
      "batch 778: loss 0.000772\n",
      "batch 779: loss 0.001018\n",
      "batch 780: loss 0.000896\n",
      "batch 781: loss 0.000691\n",
      "batch 782: loss 0.000889\n",
      "batch 783: loss 0.000908\n",
      "batch 784: loss 0.000868\n",
      "batch 785: loss 0.000680\n",
      "batch 786: loss 0.001293\n",
      "batch 787: loss 0.000791\n",
      "batch 788: loss 0.000778\n",
      "batch 789: loss 0.000848\n",
      "batch 790: loss 0.000958\n",
      "batch 791: loss 0.000904\n",
      "batch 792: loss 0.000914\n",
      "batch 793: loss 0.000984\n",
      "batch 794: loss 0.000897\n",
      "batch 795: loss 0.000894\n",
      "batch 796: loss 0.000689\n",
      "batch 797: loss 0.000753\n",
      "batch 798: loss 0.001397\n",
      "batch 799: loss 0.000788\n",
      "batch 800: loss 0.000916\n",
      "batch 801: loss 0.001021\n",
      "batch 802: loss 0.000790\n",
      "batch 803: loss 0.001157\n",
      "batch 804: loss 0.000730\n",
      "batch 805: loss 0.000705\n",
      "batch 806: loss 0.000855\n",
      "batch 807: loss 0.001307\n",
      "batch 808: loss 0.000599\n",
      "batch 809: loss 0.001069\n",
      "batch 810: loss 0.000625\n",
      "batch 811: loss 0.000628\n",
      "batch 812: loss 0.000924\n",
      "batch 813: loss 0.000736\n",
      "batch 814: loss 0.000741\n",
      "batch 815: loss 0.001178\n",
      "batch 816: loss 0.000675\n",
      "batch 817: loss 0.000969\n",
      "batch 818: loss 0.000877\n",
      "batch 819: loss 0.000970\n",
      "batch 820: loss 0.000692\n",
      "batch 821: loss 0.001133\n",
      "batch 822: loss 0.001138\n",
      "batch 823: loss 0.000865\n",
      "batch 824: loss 0.000685\n",
      "batch 825: loss 0.000658\n",
      "batch 826: loss 0.000865\n",
      "batch 827: loss 0.000960\n",
      "batch 828: loss 0.000971\n",
      "batch 829: loss 0.000628\n",
      "batch 830: loss 0.000474\n",
      "batch 831: loss 0.000654\n",
      "batch 832: loss 0.000673\n",
      "batch 833: loss 0.000631\n",
      "batch 834: loss 0.000897\n",
      "batch 835: loss 0.000797\n",
      "batch 836: loss 0.001018\n",
      "batch 837: loss 0.000923\n",
      "batch 838: loss 0.001107\n",
      "batch 839: loss 0.000705\n",
      "batch 840: loss 0.000578\n",
      "batch 841: loss 0.001266\n",
      "batch 842: loss 0.000682\n",
      "batch 843: loss 0.000705\n",
      "batch 844: loss 0.000920\n",
      "batch 845: loss 0.000656\n",
      "batch 846: loss 0.000449\n",
      "batch 847: loss 0.000906\n",
      "batch 848: loss 0.000814\n",
      "batch 849: loss 0.000747\n",
      "batch 850: loss 0.000783\n",
      "batch 851: loss 0.000974\n",
      "batch 852: loss 0.000569\n",
      "batch 853: loss 0.000902\n",
      "batch 854: loss 0.000899\n",
      "batch 855: loss 0.000713\n",
      "batch 856: loss 0.001051\n",
      "batch 857: loss 0.000707\n",
      "batch 858: loss 0.000613\n",
      "batch 859: loss 0.000481\n",
      "batch 860: loss 0.000737\n",
      "batch 861: loss 0.000796\n",
      "batch 862: loss 0.000753\n",
      "batch 863: loss 0.000798\n",
      "batch 864: loss 0.000556\n",
      "batch 865: loss 0.000706\n",
      "batch 866: loss 0.000615\n",
      "batch 867: loss 0.000822\n",
      "batch 868: loss 0.000909\n",
      "batch 869: loss 0.000684\n",
      "batch 870: loss 0.000651\n",
      "batch 871: loss 0.000384\n",
      "batch 872: loss 0.001000\n",
      "batch 873: loss 0.000940\n",
      "batch 874: loss 0.000527\n",
      "batch 875: loss 0.000927\n",
      "batch 876: loss 0.001167\n",
      "batch 877: loss 0.000715\n",
      "batch 878: loss 0.000691\n",
      "batch 879: loss 0.000723\n",
      "batch 880: loss 0.000473\n",
      "batch 881: loss 0.000530\n",
      "batch 882: loss 0.000440\n",
      "batch 883: loss 0.000751\n",
      "batch 884: loss 0.000888\n",
      "batch 885: loss 0.000769\n",
      "batch 886: loss 0.000609\n",
      "batch 887: loss 0.000950\n",
      "batch 888: loss 0.001116\n",
      "batch 889: loss 0.000661\n",
      "batch 890: loss 0.000563\n",
      "batch 891: loss 0.000916\n",
      "batch 892: loss 0.000827\n",
      "batch 893: loss 0.000923\n",
      "batch 894: loss 0.000767\n",
      "batch 895: loss 0.000624\n",
      "batch 896: loss 0.000847\n",
      "batch 897: loss 0.000972\n",
      "batch 898: loss 0.000592\n",
      "batch 899: loss 0.001092\n",
      "batch 900: loss 0.000914\n",
      "batch 901: loss 0.000749\n",
      "batch 902: loss 0.000708\n",
      "batch 903: loss 0.000738\n",
      "batch 904: loss 0.000882\n",
      "batch 905: loss 0.000531\n",
      "batch 906: loss 0.000694\n",
      "batch 907: loss 0.001082\n",
      "batch 908: loss 0.000848\n",
      "batch 909: loss 0.000926\n",
      "batch 910: loss 0.000908\n",
      "batch 911: loss 0.000693\n",
      "batch 912: loss 0.000780\n",
      "batch 913: loss 0.000812\n",
      "batch 914: loss 0.000644\n",
      "batch 915: loss 0.000569\n",
      "batch 916: loss 0.001027\n",
      "batch 917: loss 0.000893\n",
      "batch 918: loss 0.001167\n",
      "batch 919: loss 0.000888\n",
      "batch 920: loss 0.000788\n",
      "batch 921: loss 0.000998\n",
      "batch 922: loss 0.000780\n",
      "batch 923: loss 0.000625\n",
      "batch 924: loss 0.000672\n",
      "batch 925: loss 0.000674\n",
      "batch 926: loss 0.000746\n",
      "batch 927: loss 0.000670\n",
      "batch 928: loss 0.000983\n",
      "batch 929: loss 0.000893\n",
      "batch 930: loss 0.000603\n",
      "batch 931: loss 0.000840\n",
      "batch 932: loss 0.000695\n",
      "batch 933: loss 0.000707\n",
      "batch 934: loss 0.000620\n",
      "batch 935: loss 0.000840\n",
      "batch 936: loss 0.000931\n",
      "batch 937: loss 0.000781\n",
      "batch 938: loss 0.000472\n",
      "batch 939: loss 0.000741\n",
      "batch 940: loss 0.000763\n",
      "batch 941: loss 0.000935\n",
      "batch 942: loss 0.000430\n",
      "batch 943: loss 0.000602\n",
      "batch 944: loss 0.000865\n",
      "batch 945: loss 0.000891\n",
      "batch 946: loss 0.000754\n",
      "batch 947: loss 0.000424\n",
      "batch 948: loss 0.000891\n",
      "batch 949: loss 0.000928\n",
      "batch 950: loss 0.000901\n",
      "batch 951: loss 0.000736\n",
      "batch 952: loss 0.000635\n",
      "batch 953: loss 0.000405\n",
      "batch 954: loss 0.000597\n",
      "batch 955: loss 0.000912\n",
      "batch 956: loss 0.000541\n",
      "batch 957: loss 0.000781\n",
      "batch 958: loss 0.000772\n",
      "batch 959: loss 0.000765\n",
      "batch 960: loss 0.000847\n",
      "batch 961: loss 0.000948\n",
      "batch 962: loss 0.000782\n",
      "batch 963: loss 0.000896\n",
      "batch 964: loss 0.000754\n",
      "batch 965: loss 0.000696\n",
      "batch 966: loss 0.000564\n",
      "batch 967: loss 0.000703\n",
      "batch 968: loss 0.000625\n",
      "batch 969: loss 0.000636\n",
      "batch 970: loss 0.000564\n",
      "batch 971: loss 0.000587\n",
      "batch 972: loss 0.000610\n",
      "batch 973: loss 0.000518\n",
      "batch 974: loss 0.000582\n",
      "batch 975: loss 0.000934\n",
      "batch 976: loss 0.000733\n",
      "batch 977: loss 0.000626\n",
      "batch 978: loss 0.000598\n",
      "batch 979: loss 0.000835\n",
      "batch 980: loss 0.000907\n",
      "batch 981: loss 0.000650\n",
      "batch 982: loss 0.000420\n",
      "batch 983: loss 0.000702\n",
      "batch 984: loss 0.000541\n",
      "batch 985: loss 0.000840\n",
      "batch 986: loss 0.000601\n",
      "batch 987: loss 0.000669\n",
      "batch 988: loss 0.000708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 989: loss 0.000666\n",
      "batch 990: loss 0.000797\n",
      "batch 991: loss 0.000649\n",
      "batch 992: loss 0.000549\n",
      "batch 993: loss 0.000771\n",
      "batch 994: loss 0.000696\n",
      "batch 995: loss 0.000417\n",
      "batch 996: loss 0.000734\n",
      "batch 997: loss 0.000444\n",
      "batch 998: loss 0.000519\n",
      "batch 999: loss 0.000359\n"
     ]
    }
   ],
   "source": [
    "data_prep_ORL()\n",
    "# print(YBigData)\n",
    "# print(imageBigDataORL)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(imageBigDataORL, YBigData, test_size=0.3, random_state=42)\n",
    "# Y_train.reshape((1, Y_train.shape[0]))\n",
    "# Y_test.reshape((1, Y_test.shape[0]))\n",
    "# enc = OneHotEncoder(handle_unknown='ignore')\n",
    "# Y_train_reshaped = np.reshape(Y_train, (Y_train.shape[0], 1))\n",
    "# enc.fit(Y_train_reshaped)\n",
    "# onehot_Y_train_reshaped = enc.transform(Y_train_reshaped).toarray()\n",
    "# Y_test_reshaped = np.reshape(Y_test, (Y_test.shape[0], 1))\n",
    "# onehot_Y_test_reshaped = enc.transform(Y_test_reshaped).toarray()\n",
    "# onehot_Y_test = np.reshape(onehot_Y_test_reshaped, (one))\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(Y_train)\n",
    "# print(Y_test)\n",
    "# print(Y_train.T)\n",
    "# print(np.reshape(Y_train, (Y_train.shape[0], 1)))\n",
    "# Y_train_reshaped = np.reshape(Y_train, (Y_train.shape[0], 1))\n",
    "# print(Y_train_reshaped)\n",
    "# enc = OneHotEncoder(handle_unknown='ignore')\n",
    "# enc.fit(Y_train_reshaped)\n",
    "# onehot_Y_train_reshaped = enc.transform(Y_train_reshaped).toarray()\n",
    "# Y_test_reshaped = np.reshape(Y_test, (Y_test.shape[0], 1))\n",
    "# onehot_Y_test_reshaped = enc.transform(Y_test_reshaped).toarray()\n",
    "# print(Y_train_reshaped.shape)\n",
    "# print(onehot_Y_train_reshaped.shape)\n",
    "# print(onehot_y_train_reshaped)\n",
    "num_batches = 1000\n",
    "batch_size = 50\n",
    "learning_rate = 0.00001\n",
    "\n",
    "# global_step = tf.Variable(0, trainable=False)\n",
    "# starter_learning_rate = 0.1\n",
    "# learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "#                                            100000, 0.96, staircase=True)\n",
    "# # Passing global_step to minimize() will increment it at each step.\n",
    "# learning_step = (\n",
    "#     tf.train.GradientDescentOptimizer(learning_rate)\n",
    "#     .minimize(...my loss..., global_step=global_step)\n",
    "# )\n",
    "\n",
    "model = simplePerceptron()\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=get_lr)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) #.minimize(loss, global_step = global_step)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = get_batch(batch_size)\n",
    "    X = X.astype(np.float32)\n",
    "    y = y.astype(np.int32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_logit_pred = model(tf.convert_to_tensor(X))\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_logit_pred)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "#     num_Y_samples = np.shape(Y_test)[0]\n",
    "#     y_pred = model.predict(X_test).numpy()\n",
    "#     print(y_pred)\n",
    "#     print(\"test accuracy: %f\" % (sum(y_pred == Y_test) / num_Y_samples))\n",
    "#     if sum(y_pred == Y_test) / num_Y_samples > 0.9:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T01:20:52.461379Z",
     "start_time": "2019-01-30T01:20:52.418656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3 28  3 21  9 38 32  9  4 12 31 36  5 10 13 24 12 37 23 33 10  1 39 27\n",
      "  0  3 11 22 21 10 14 28  0 30 23 11  4 34  0 12 34  5 10  2  8 38 12 18\n",
      "  2 17  4 32 33 38 37  4 22 17  3 15 12 29 25  7 10 19 16 24 17 17 20 38\n",
      "  0  4  6 24 38  2 12 10 12  1  8 25 27 22 26 30 10 27 15 33 11 22 10 31\n",
      " 34 29 19 10 28 11 11 23 15  0 11 13 18  6 13  3  3  5 29  6  1 28 28 28]\n",
      "test accuracy: 0.525000\n"
     ]
    }
   ],
   "source": [
    "num_Y_samples = np.shape(Y_test)[0]\n",
    "y_pred = model.predict(X_test).numpy()\n",
    "print(y_pred)\n",
    "print(\"test accuracy: %f\" % (sum(y_pred == Y_test) / num_Y_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
