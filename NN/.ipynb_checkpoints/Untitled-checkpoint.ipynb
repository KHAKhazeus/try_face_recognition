{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-30T13:47:39.178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 29.  24.  21. ... 255. 254. 255.]\n",
      " [ 79.  17.  17. ... 234. 237. 232.]\n",
      " [255. 254. 255. ... 129. 124. 136.]\n",
      " ...\n",
      " [ 16.  21.  20. ... 255. 255. 255.]\n",
      " [ 19.  15.  13. ... 255. 255. 255.]\n",
      " [ 37.  38.  41. ... 251. 253. 254.]]\n",
      "[[[[0.11372549]\n",
      "   [0.09411765]\n",
      "   [0.08235294]\n",
      "   ...\n",
      "   [0.09019608]\n",
      "   [0.08235294]\n",
      "   [0.11372549]]\n",
      "\n",
      "  [[0.10588235]\n",
      "   [0.09019608]\n",
      "   [0.09019608]\n",
      "   ...\n",
      "   [0.09019608]\n",
      "   [0.09411765]\n",
      "   [0.09019608]]\n",
      "\n",
      "  [[0.10980392]\n",
      "   [0.09803922]\n",
      "   [0.09019608]\n",
      "   ...\n",
      "   [0.08235294]\n",
      "   [0.09411765]\n",
      "   [0.09019608]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.6392157 ]\n",
      "   [0.6666667 ]\n",
      "   [0.6666667 ]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]]\n",
      "\n",
      "  [[0.63529414]\n",
      "   [0.6666667 ]\n",
      "   [0.67058825]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]]\n",
      "\n",
      "  [[0.61960787]\n",
      "   [0.67058825]\n",
      "   [0.67058825]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [0.99607843]\n",
      "   [1.        ]]]\n",
      "\n",
      "\n",
      " [[[0.30980393]\n",
      "   [0.06666667]\n",
      "   [0.06666667]\n",
      "   ...\n",
      "   [0.21960784]\n",
      "   [0.5254902 ]\n",
      "   [0.91764706]]\n",
      "\n",
      "  [[0.1254902 ]\n",
      "   [0.05882353]\n",
      "   [0.06666667]\n",
      "   ...\n",
      "   [0.10196079]\n",
      "   [0.3137255 ]\n",
      "   [0.9490196 ]]\n",
      "\n",
      "  [[0.08627451]\n",
      "   [0.05882353]\n",
      "   [0.0627451 ]\n",
      "   ...\n",
      "   [0.07450981]\n",
      "   [0.24705882]\n",
      "   [0.95686275]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.99215686]\n",
      "   [0.8862745 ]\n",
      "   [0.60784316]\n",
      "   ...\n",
      "   [0.85882354]\n",
      "   [0.8352941 ]\n",
      "   [0.85882354]]\n",
      "\n",
      "  [[0.9843137 ]\n",
      "   [0.87058824]\n",
      "   [0.5882353 ]\n",
      "   ...\n",
      "   [0.92156863]\n",
      "   [0.8901961 ]\n",
      "   [0.9098039 ]]\n",
      "\n",
      "  [[0.9411765 ]\n",
      "   [0.79607844]\n",
      "   [0.52156866]\n",
      "   ...\n",
      "   [0.91764706]\n",
      "   [0.92941177]\n",
      "   [0.9098039 ]]]\n",
      "\n",
      "\n",
      " [[[1.        ]\n",
      "   [0.99607843]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [0.1764706 ]\n",
      "   [0.1764706 ]\n",
      "   [0.18039216]]\n",
      "\n",
      "  [[1.        ]\n",
      "   [1.        ]\n",
      "   [0.9843137 ]\n",
      "   ...\n",
      "   [0.16078432]\n",
      "   [0.16078432]\n",
      "   [0.16470589]]\n",
      "\n",
      "  [[1.        ]\n",
      "   [0.99215686]\n",
      "   [0.9372549 ]\n",
      "   ...\n",
      "   [0.15686275]\n",
      "   [0.16078432]\n",
      "   [0.14901961]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.        ]\n",
      "   [0.99607843]\n",
      "   [0.99607843]\n",
      "   ...\n",
      "   [0.5686275 ]\n",
      "   [0.75686276]\n",
      "   [0.9882353 ]]\n",
      "\n",
      "  [[0.99607843]\n",
      "   [0.99607843]\n",
      "   [0.972549  ]\n",
      "   ...\n",
      "   [0.52156866]\n",
      "   [0.627451  ]\n",
      "   [0.9411765 ]]\n",
      "\n",
      "  [[0.85882354]\n",
      "   [0.6156863 ]\n",
      "   [0.48235294]\n",
      "   ...\n",
      "   [0.5058824 ]\n",
      "   [0.4862745 ]\n",
      "   [0.53333336]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.0627451 ]\n",
      "   [0.08235294]\n",
      "   [0.07843138]\n",
      "   ...\n",
      "   [0.09411765]\n",
      "   [0.23137255]\n",
      "   [0.8862745 ]]\n",
      "\n",
      "  [[0.04705882]\n",
      "   [0.07843138]\n",
      "   [0.07843138]\n",
      "   ...\n",
      "   [0.07058824]\n",
      "   [0.09019608]\n",
      "   [0.4745098 ]]\n",
      "\n",
      "  [[0.04313726]\n",
      "   [0.07058824]\n",
      "   [0.07450981]\n",
      "   ...\n",
      "   [0.06666667]\n",
      "   [0.08627451]\n",
      "   [0.27058825]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]]\n",
      "\n",
      "  [[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]]\n",
      "\n",
      "  [[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]]]\n",
      "\n",
      "\n",
      " [[[0.07450981]\n",
      "   [0.05882353]\n",
      "   [0.05098039]\n",
      "   ...\n",
      "   [0.03529412]\n",
      "   [0.04313726]\n",
      "   [0.07843138]]\n",
      "\n",
      "  [[0.07058824]\n",
      "   [0.05882353]\n",
      "   [0.04705882]\n",
      "   ...\n",
      "   [0.05098039]\n",
      "   [0.0627451 ]\n",
      "   [0.09411765]]\n",
      "\n",
      "  [[0.07058824]\n",
      "   [0.05882353]\n",
      "   [0.04705882]\n",
      "   ...\n",
      "   [0.06666667]\n",
      "   [0.07843138]\n",
      "   [0.10588235]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [0.99607843]\n",
      "   [0.96862745]]\n",
      "\n",
      "  [[1.        ]\n",
      "   [0.99607843]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]]\n",
      "\n",
      "  [[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]]]\n",
      "\n",
      "\n",
      " [[[0.14509805]\n",
      "   [0.14901961]\n",
      "   [0.16078432]\n",
      "   ...\n",
      "   [0.21568628]\n",
      "   [0.21176471]\n",
      "   [0.22745098]]\n",
      "\n",
      "  [[0.15686275]\n",
      "   [0.1882353 ]\n",
      "   [0.20784314]\n",
      "   ...\n",
      "   [0.19607843]\n",
      "   [0.20392157]\n",
      "   [0.20392157]]\n",
      "\n",
      "  [[0.16470589]\n",
      "   [0.1882353 ]\n",
      "   [0.20392157]\n",
      "   ...\n",
      "   [0.1882353 ]\n",
      "   [0.19607843]\n",
      "   [0.19607843]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [0.9882353 ]\n",
      "   [0.972549  ]\n",
      "   [0.972549  ]]\n",
      "\n",
      "  [[1.        ]\n",
      "   [1.        ]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [0.99607843]\n",
      "   [0.99607843]\n",
      "   [0.99607843]]\n",
      "\n",
      "  [[0.99607843]\n",
      "   [0.99607843]\n",
      "   [1.        ]\n",
      "   ...\n",
      "   [0.9843137 ]\n",
      "   [0.99215686]\n",
      "   [0.99607843]]]]\n",
      "= = = = = = > > > > > >  step: 0 loss: 2.4161 accuracy: 0.28\n",
      "= = = = = = > > > > > >  step: 1 loss: 2.1379 accuracy: 0.34\n",
      "= = = = = = > > > > > >  step: 2 loss: 2.1769 accuracy: 0.33\n",
      "= = = = = = > > > > > >  step: 3 loss: 1.8878 accuracy: 0.34\n",
      "= = = = = = > > > > > >  step: 4 loss: 1.4054 accuracy: 0.38\n",
      "= = = = = = > > > > > >  step: 5 loss: 1.3244 accuracy: 0.42\n",
      "= = = = = = > > > > > >  step: 6 loss: 1.4087 accuracy: 0.46\n",
      "= = = = = = > > > > > >  step: 7 loss: 0.8694 accuracy: 0.50\n",
      "= = = = = = > > > > > >  step: 8 loss: 0.9060 accuracy: 0.53\n",
      "= = = = = = > > > > > >  step: 9 loss: 0.6200 accuracy: 0.56\n"
     ]
    }
   ],
   "source": [
    "#coding:utf-8\n",
    "\"\"\"\n",
    "python 3\n",
    "tensorflow 1.1\n",
    "matplotlib 2.02\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "learning_rate = 0.0001\n",
    "batch_size=5\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "def get_batch(size):\n",
    "    index = np.random.randint(0, np.shape(X_train)[0], size)\n",
    "    return imageBigDataORL[index, :], Big_Y[index]\n",
    "\n",
    "\n",
    "tf.set_random_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "#50\n",
    "\n",
    "imageBigDataORL = np.zeros((165, 64*64))\n",
    "YBigData = np.zeros((165,1))\n",
    "\n",
    "#read_in\n",
    "\n",
    "for people in range(1,16):\n",
    "    for face in range(1,12):\n",
    "        if(people >= 10):\n",
    "            path = './Yale2/subject%d_%d.bmp' % (people, face)\n",
    "        else:\n",
    "            path = './Yale2/subject0%d_%d.bmp' % (people, face)\n",
    "        oriImage = PIL.Image.open(path)\n",
    "        oriImage = oriImage.resize((64,64))\n",
    "        imageArray = np.array(oriImage)\n",
    "        imageVec = np.reshape(imageArray, imageArray.shape[0] * imageArray.shape[1])\n",
    "        # print(imageArray.shape)\n",
    "        # print(imageBigDataORL.shape)\n",
    "#             print((people - 1) * 10 + face)\n",
    "        imageBigDataORL[(people - 1) * 10 + face - 1] = imageVec\n",
    "        YBigData[(people - 1) * 11 + face - 1] = people - 1\n",
    "\n",
    "# onehot and other pre-process\n",
    "\n",
    "\n",
    "# print(imageBigDataORL)\n",
    "# print(YBigData.max())\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(YBigData)\n",
    "Big_Y = enc.transform(YBigData).toarray()\n",
    "# print(Big_Y)\n",
    "\n",
    "# divide train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(imageBigDataORL, Big_Y, test_size=0.2, random_state=50)\n",
    "\n",
    "\n",
    "np.random.seed(100)\n",
    "X_train = np.random.permutation(imageBigDataORL)\n",
    "np.random.seed(100)\n",
    "Y_train = np.random.permutation(Big_Y)\n",
    "X_test = imageBigDataORL[0:50,:]\n",
    "Y_test = Big_Y[0:50]\n",
    "np.random.seed(200)\n",
    "X_test = np.random.permutation(X_test)\n",
    "np.random.seed(200)\n",
    "Y_test = np.random.permutation(Y_test)\n",
    "# # plot one example\n",
    "\n",
    "# # print(X_train.shape)     # (55000, 28 * 28)\n",
    "# # print(Y_train.shape)   # (55000, 10)\n",
    "# # plt.imshow(X_train[0].reshape((100, 100)), cmap='gray')\n",
    "# # plt.title('%i' % (np.argmax(Y_train[0]) + 1)); plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#加载数据\n",
    "def read_data(filename):\n",
    "    with open(filename,'rb') as f:\n",
    "        #记载matlab文件\n",
    "        dict = sio.loadmat(f)\n",
    "    return dict['fea'],dict['gnd']\n",
    "\n",
    "train_data,train_labels = read_data('Yale_64x64.mat')\n",
    "#将标签转为0-14\n",
    "train_labels = train_labels-1\n",
    "\n",
    "#shuffle data\n",
    "np.random.seed(100)\n",
    "train_data = np.random.permutation(train_data)\n",
    "np.random.seed(100)\n",
    "train_labels = np.random.permutation(train_labels)\n",
    "test_data = train_data[0:50,:]\n",
    "test_labels = train_labels[0:50]\n",
    "np.random.seed(200)\n",
    "test_data = np.random.permutation(test_data)\n",
    "np.random.seed(200)\n",
    "test_labels = np.random.permutation(test_labels)\n",
    "\n",
    "# # plt.imshow(train_data[0])\n",
    "\n",
    "#将标签转为one_hot类型\n",
    "def label_to_one_hot(labels_dense, num_classes=15):\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "#将图片转为灰度图\n",
    "def to4d(img):\n",
    "    return img.reshape(img.shape[0],64,64,1).astype(np.float32)/255\n",
    "\n",
    "train_data = to4d(train_data)\n",
    "train_labels = label_to_one_hot(train_labels,15)\n",
    "test_data = to4d(test_data)\n",
    "test_labels = label_to_one_hot(test_labels,15)\n",
    "\n",
    "print(X_train)\n",
    "\n",
    "X_train = to4d(X_train)\n",
    "X_test = to4d(X_test)\n",
    "print(X_train)\n",
    "\n",
    "xs = tf.placeholder(tf.float32,[None,64,64,1])\n",
    "ys = tf.placeholder(tf.float32,[None,15])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "#开始构建卷积神经网络\n",
    "conv1 = tf.layers.conv2d(inputs=xs,filters=32,kernel_size=2,strides=1,padding='same',activation=tf.nn.relu)\n",
    "pool1 = tf.layers.max_pooling2d(conv1,pool_size=2,strides=2)\n",
    "conv2 = tf.layers.conv2d(pool1,filters=72,kernel_size=2,strides=1,padding='same',activation=tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(conv2,pool_size=2,strides=2)\n",
    "flat = tf.reshape(pool2,[-1,16*16*72])\n",
    "dense = tf.layers.dense(flat,512,tf.nn.relu)\n",
    "dropout = tf.nn.dropout(dense,keep_prob)\n",
    "output = tf.layers.dense(dropout,15)\n",
    "\n",
    "#计算loss\n",
    "loss = tf.losses.softmax_cross_entropy(onehot_labels=ys,logits=output)\n",
    "train = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "#返回两个参数一个train_opt,一个acc\n",
    "accuracy = tf.metrics.accuracy(labels=tf.argmax(ys,axis=1),predictions=tf.argmax(output,axis=1))[1]\n",
    "\n",
    "# print(X_train)\n",
    "# print(Y_train)\n",
    "with tf.Session() as sess:\n",
    "    init = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "    sess.run(init)\n",
    "    for step in range(1000):\n",
    "        i = 0\n",
    "        while i < len(train_data):\n",
    "            start = i\n",
    "            end = i+batch_size\n",
    "            batch_x = np.array(X_train[start:end])\n",
    "            batch_y = np.array(Y_train[start:end])\n",
    "            _,c = sess.run([train,loss],feed_dict={xs:batch_x,ys:batch_y,keep_prob:0.75})\n",
    "            i += batch_size\n",
    "        if step % 1 ==0:\n",
    "            acc = sess.run(accuracy,feed_dict={xs:X_test,ys:Y_test,keep_prob:1})\n",
    "            print('= = = = = = > > > > > > ','step:',step,'loss: %.4f'%c,'accuracy: %.2f' %acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-30T13:47:39.192Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "learning_rate = 0.0001\n",
    "batch_size=5\n",
    "\n",
    "#加载数据\n",
    "def read_data(filename):\n",
    "    with open(filename,'rb') as f:\n",
    "        #记载matlab文件\n",
    "        dict = sio.loadmat(f)\n",
    "    return dict['fea'],dict['gnd']\n",
    "\n",
    "train_data,train_labels = read_data('Yale_64x64.mat')\n",
    "#将标签转为0-14\n",
    "train_labels = train_labels-1\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "# print(train_data)\n",
    "print(train_labels.dtype)\n",
    "plt.imshow(train_data[0].reshape([64,64]))\n",
    "\n",
    "#shuffle data\n",
    "np.random.seed(100)\n",
    "train_data = np.random.permutation(train_data)\n",
    "np.random.seed(100)\n",
    "train_labels = np.random.permutation(train_labels)\n",
    "test_data = train_data[0:50,:]\n",
    "test_labels = train_labels[0:50]\n",
    "np.random.seed(200)\n",
    "test_data = np.random.permutation(test_data)\n",
    "np.random.seed(200)\n",
    "test_labels = np.random.permutation(test_labels)\n",
    "\n",
    "#将标签转为one_hot类型\n",
    "def label_to_one_hot(labels_dense, num_classes=15):\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    print(num_labels)\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    print(index_offset)\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    print(labels_dense.ravel())\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "#将图片转为灰度图\n",
    "def to4d(img):\n",
    "    return img.reshape(img.shape[0],64,64,1).astype(np.float32)/255\n",
    "\n",
    "train_data = to4d(train_data)\n",
    "train_labels = label_to_one_hot(train_labels,15)\n",
    "test_data = to4d(test_data)\n",
    "test_labels = label_to_one_hot(test_labels,15)\n",
    "plt.figure()\n",
    "plt.imshow(train_data[0,:,:,0])\n",
    "plt.title(np.argmax(train_labels[0]))\n",
    "\n",
    "xs = tf.placeholder(tf.float32,[None,64,64,1])\n",
    "ys = tf.placeholder(tf.float32,[None,15])\n",
    "keep_prob = tf.placeholder(tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-30T13:47:39.194Z"
    }
   },
   "outputs": [],
   "source": [
    "#开始构建卷积神经网络\n",
    "conv1 = tf.layers.conv2d(inputs=xs,filters=32,kernel_size=2,strides=1,padding='same',activation=tf.nn.relu)\n",
    "pool1 = tf.layers.max_pooling2d(conv1,pool_size=2,strides=2)\n",
    "conv2 = tf.layers.conv2d(pool1,filters=72,kernel_size=2,strides=1,padding='same',activation=tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(conv2,pool_size=2,strides=2)\n",
    "flat = tf.reshape(pool2,[-1,16*16*72])\n",
    "dense = tf.layers.dense(flat,512,tf.nn.relu)\n",
    "dropout = tf.nn.dropout(dense,keep_prob)\n",
    "output = tf.layers.dense(dropout,15)\n",
    "\n",
    "#计算loss\n",
    "loss = tf.losses.softmax_cross_entropy(onehot_labels=ys,logits=output)\n",
    "train = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "#返回两个参数一个train_opt,一个acc\n",
    "accuracy = tf.metrics.accuracy(labels=tf.argmax(ys,axis=1),predictions=tf.argmax(output,axis=1))[1]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "    sess.run(init)\n",
    "    for step in range(1000):\n",
    "        i = 0\n",
    "        while i < len(train_data):\n",
    "            start = i\n",
    "            end = i+batch_size\n",
    "            batch_x = np.array(train_data[start:end])\n",
    "            batch_y = np.array(train_labels[start:end])\n",
    "            _,c = sess.run([train,loss],feed_dict={xs:batch_x,ys:batch_y,keep_prob:0.75})\n",
    "            i += batch_size\n",
    "        if step % 1 ==0:\n",
    "            acc = sess.run(accuracy,feed_dict={xs:test_data,ys:test_labels,keep_prob:1})\n",
    "            print('= = = = = = > > > > > > ','step:',step,'loss: %.4f'%c,'accuracy: %.2f' %acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-30T13:47:39.199Z"
    }
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\"\"\"\n",
    "python 3\n",
    "tensorflow 1.1\n",
    "matplotlib 2.02\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "learning_rate = 0.0001\n",
    "batch_size=5\n",
    "\n",
    "train_data = np.zeros((165, 100*100))\n",
    "train_labels = np.zeros((165,1), dtype= np.uint8)\n",
    "\n",
    "for people in range(1,16):\n",
    "    for face in range(1,12):\n",
    "        if(people >= 10):\n",
    "            path = './Yale2/subject%d_%d.bmp' % (people, face)\n",
    "        else:\n",
    "            path = './Yale2/subject0%d_%d.bmp' % (people, face)\n",
    "        oriImage = PIL.Image.open(path)\n",
    "#         oriImage = oriImage.resize((64,64))\n",
    "        imageArray = np.array(oriImage)\n",
    "#         imageArray = imageArray.T\n",
    "        imageVec = np.reshape(imageArray, imageArray.shape[0] * imageArray.shape[1])\n",
    "        # print(imageArray.shape)\n",
    "        # print(imageBigDataORL.shape)\n",
    "#             print((people - 1) * 10 + face)\n",
    "        train_data[(people - 1) * 11 + face - 1] = imageVec\n",
    "        train_labels[(people - 1) * 11 + face - 1] = people\n",
    "    \n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "plt.figure()\n",
    "plt.imshow(train_data[0].reshape([100,100]))\n",
    "\n",
    "train_labels = train_labels - 1\n",
    "\n",
    "np.random.seed(100)\n",
    "train_data = np.random.permutation(train_data[50:,:])\n",
    "np.random.seed(100)\n",
    "train_labels = np.random.permutation(train_labels[50:])\n",
    "test_data = train_data[0:50,:]\n",
    "test_labels = train_labels[0:50]\n",
    "np.random.seed(200)\n",
    "test_data = np.random.permutation(test_data)\n",
    "np.random.seed(200)\n",
    "test_labels = np.random.permutation(test_labels)\n",
    "\n",
    "#将标签转为one_hot类型\n",
    "def label_to_one_hot(labels_dense, num_classes=15):\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    print(num_labels)\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    print(index_offset)\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    print(labels_dense.ravel())\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "#将图片转为灰度图\n",
    "def to4d(img):\n",
    "    return img.reshape(img.shape[0],100,100,1).astype(np.float32)/255\n",
    "\n",
    "train_data = to4d(train_data)\n",
    "train_labels = label_to_one_hot(train_labels,15)\n",
    "test_data = to4d(test_data)\n",
    "test_labels = label_to_one_hot(test_labels,15)\n",
    "plt.figure()\n",
    "plt.imshow(train_data[0,:,:,0])\n",
    "plt.title(np.argmax(train_labels[0]))\n",
    "\n",
    "xs = tf.placeholder(tf.float32,[None,100,100,1])\n",
    "ys = tf.placeholder(tf.float32,[None,15])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "conv1 = tf.layers.conv2d(inputs=xs,filters=32,kernel_size=2,strides=1,padding='same',activation=tf.nn.relu)\n",
    "pool1 = tf.layers.max_pooling2d(conv1,pool_size=2,strides=2)\n",
    "conv2 = tf.layers.conv2d(pool1,filters=72,kernel_size=2,strides=1,padding='same',activation=tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(conv2,pool_size=2,strides=2)\n",
    "flat = tf.reshape(pool2,[-1,25*25*72])\n",
    "dense = tf.layers.dense(flat,512,tf.nn.relu)\n",
    "dropout = tf.nn.dropout(dense,keep_prob)\n",
    "output = tf.layers.dense(dropout,15)\n",
    "\n",
    "#计算loss\n",
    "loss = tf.losses.softmax_cross_entropy(onehot_labels=ys,logits=output)\n",
    "train = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "#返回两个参数一个train_opt,一个acc\n",
    "accuracy = tf.metrics.accuracy(labels=tf.argmax(ys,axis=1),predictions=tf.argmax(output,axis=1))[1]\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "sess.run(init)\n",
    "for step in range(122):\n",
    "    i = 0\n",
    "    while i < len(train_data):\n",
    "        start = i\n",
    "        end = i+batch_size\n",
    "        batch_x = np.array(train_data[start:end])\n",
    "        batch_y = np.array(train_labels[start:end])\n",
    "        _,c = sess.run([train,loss],feed_dict={xs:batch_x,ys:batch_y,keep_prob:0.75})\n",
    "        i += batch_size\n",
    "    if step % 1 ==0:\n",
    "        acc = sess.run(accuracy,feed_dict={xs:test_data,ys:test_labels,keep_prob:1})\n",
    "        print('= = = = = = > > > > > > ','step:',step,'loss: %.4f'%c,'accuracy: %.2f' %acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-30T13:47:39.204Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "try: from sklearn.manifold import TSNE; HAS_SK = True\n",
    "except: HAS_SK = False; print('\\nPlease install sklearn for layer visualization\\n')\n",
    "def plot_with_labels(lowDWeights, labels):\n",
    "    plt.cla(); X, Y = lowDWeights[:, 0], lowDWeights[:, 1]\n",
    "    for x, y, s in zip(X, Y, labels):\n",
    "        c = cm.rainbow(int(255 * s / 15)); plt.text(x, y, s, backgroundcolor=c, fontsize=9)\n",
    "    plt.xlim(X.min(), X.max()); plt.ylim(Y.min(), Y.max()); plt.title('Visualize last layer'); plt.show(); plt.pause(0.01)\n",
    "accuracy_, flat_representation, output_y = sess.run([accuracy, flat,output], {xs: test_data, ys: test_labels, keep_prob:1.0})\n",
    "print('Step:', step, '| train loss: %f' % loss_, '| test accuracy: %f' % accuracy_)\n",
    "\n",
    "if HAS_SK:\n",
    "    # Visualization of trained flatten layer (T-SNE)\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000); plot_only = test_labels.shape[0]\n",
    "    low_dim_embs = tsne.fit_transform(flat_representation[:plot_only, :])\n",
    "    labels = np.argmax(test_labels, axis=1)[:plot_only]; plot_with_labels(low_dim_embs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
