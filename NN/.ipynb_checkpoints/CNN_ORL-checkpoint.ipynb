{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T07:08:22.946346Z",
     "start_time": "2019-01-30T07:08:16.113450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 10304)\n",
      "(280, 40)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2640becd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "def get_batch(size):\n",
    "    index = np.random.randint(0, np.shape(X_train)[0], size)\n",
    "    return X_train[index, :], Y_train[index]\n",
    "\n",
    "\n",
    "tf.set_random_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "#50\n",
    "BATCH_SIZE = 70\n",
    "# LR = 0.001              # learning rate\n",
    "\n",
    "imageBigDataORL = np.zeros((400, 112*92))\n",
    "YBigData = np.zeros((400,1))\n",
    "\n",
    "#read_in\n",
    "\n",
    "for people in range(1,41):\n",
    "    for face in range(1,11):\n",
    "        path = './ORL/s%d_%d.bmp' % (people, face)\n",
    "        oriImage = PIL.Image.open(path)\n",
    "        imageArray = np.array(oriImage)\n",
    "#             print(imageArray)\n",
    "        imageVec = np.reshape(imageArray, imageArray.shape[0] * imageArray.shape[1])\n",
    "        # print(imageArray.shape)\n",
    "        # print(imageBigDataORL.shape)\n",
    "#             print((people - 1) * 10 + face)\n",
    "        imageBigDataORL[(people - 1) * 10 + face - 1] = imageVec\n",
    "        YBigData[(people - 1) * 10 + face - 1] = people\n",
    "\n",
    "# onehot and other pre-process\n",
    "\n",
    "\n",
    "# print(imageBigDataORL)\n",
    "# print(YBigData)\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(YBigData)\n",
    "Big_Y = enc.transform(YBigData).toarray()\n",
    "# print(Big_Y)\n",
    "\n",
    "# divide train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(imageBigDataORL, Big_Y, test_size=0.3, random_state=50)\n",
    "\n",
    "# plot one example\n",
    "\n",
    "print(X_train.shape)     # (55000, 28 * 28)\n",
    "print(Y_train.shape)   # (55000, 10)\n",
    "plt.imshow(X_train[0].reshape((112, 92)), cmap='gray')\n",
    "plt.title('%i' % (np.argmax(Y_train[0]) + 1)); plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T07:25:08.244256Z",
     "start_time": "2019-01-30T07:08:22.948745Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 | train loss: 31.155779 | test accuracy: 0.016667\n",
      "Step: 1 | train loss: 222.441757 | test accuracy: 0.025000\n",
      "Step: 2 | train loss: 117.342995 | test accuracy: 0.027778\n",
      "Step: 3 | train loss: 24.367971 | test accuracy: 0.037500\n",
      "Step: 4 | train loss: 11.911387 | test accuracy: 0.043333\n",
      "Step: 5 | train loss: 7.683240 | test accuracy: 0.040278\n",
      "Step: 6 | train loss: 6.322004 | test accuracy: 0.041667\n",
      "Step: 7 | train loss: 4.110286 | test accuracy: 0.044792\n",
      "Step: 8 | train loss: 3.816181 | test accuracy: 0.042593\n",
      "Step: 9 | train loss: 3.982955 | test accuracy: 0.043333\n",
      "Step: 10 | train loss: 3.551394 | test accuracy: 0.044697\n",
      "Step: 11 | train loss: 3.549396 | test accuracy: 0.044444\n",
      "Step: 12 | train loss: 3.466455 | test accuracy: 0.045513\n",
      "Step: 13 | train loss: 3.503988 | test accuracy: 0.045833\n",
      "Step: 14 | train loss: 3.445217 | test accuracy: 0.043889\n",
      "Step: 15 | train loss: 3.126629 | test accuracy: 0.047396\n",
      "Step: 16 | train loss: 3.020970 | test accuracy: 0.052451\n",
      "Step: 17 | train loss: 2.877986 | test accuracy: 0.064352\n",
      "Step: 18 | train loss: 2.321419 | test accuracy: 0.080263\n",
      "Step: 19 | train loss: 2.098214 | test accuracy: 0.099167\n",
      "Step: 20 | train loss: 1.614855 | test accuracy: 0.121032\n",
      "Step: 21 | train loss: 1.208649 | test accuracy: 0.146970\n",
      "Step: 22 | train loss: 0.728743 | test accuracy: 0.169203\n",
      "Step: 23 | train loss: 0.812547 | test accuracy: 0.193056\n",
      "Step: 24 | train loss: 0.466433 | test accuracy: 0.217333\n",
      "Step: 25 | train loss: 0.371219 | test accuracy: 0.237500\n",
      "Step: 26 | train loss: 0.301985 | test accuracy: 0.257407\n",
      "Step: 27 | train loss: 0.292237 | test accuracy: 0.280060\n",
      "Step: 28 | train loss: 0.128748 | test accuracy: 0.300862\n",
      "Step: 29 | train loss: 0.144330 | test accuracy: 0.320556\n",
      "Step: 30 | train loss: 0.103786 | test accuracy: 0.338172\n",
      "Step: 31 | train loss: 0.126267 | test accuracy: 0.354427\n",
      "Step: 32 | train loss: 0.168097 | test accuracy: 0.369444\n",
      "Step: 33 | train loss: 0.330675 | test accuracy: 0.384314\n",
      "Step: 34 | train loss: 0.030096 | test accuracy: 0.396429\n",
      "Step: 35 | train loss: 0.351788 | test accuracy: 0.408102\n",
      "Step: 36 | train loss: 0.028921 | test accuracy: 0.420721\n",
      "Step: 37 | train loss: 0.029266 | test accuracy: 0.432456\n",
      "Step: 38 | train loss: 0.085419 | test accuracy: 0.443803\n",
      "Step: 39 | train loss: 0.025694 | test accuracy: 0.454792\n",
      "Step: 40 | train loss: 0.042240 | test accuracy: 0.465650\n",
      "Step: 41 | train loss: 0.042388 | test accuracy: 0.476190\n",
      "Step: 42 | train loss: 0.024211 | test accuracy: 0.486434\n",
      "Step: 43 | train loss: 0.023041 | test accuracy: 0.496212\n",
      "Step: 44 | train loss: 0.005462 | test accuracy: 0.505185\n",
      "Step: 45 | train loss: 0.005111 | test accuracy: 0.513587\n",
      "Step: 46 | train loss: 0.006991 | test accuracy: 0.521631\n",
      "Step: 47 | train loss: 0.006467 | test accuracy: 0.529514\n",
      "Step: 48 | train loss: 0.052905 | test accuracy: 0.537585\n",
      "Step: 49 | train loss: 0.006452 | test accuracy: 0.545833\n",
      "Step: 50 | train loss: 0.068597 | test accuracy: 0.553595\n",
      "Step: 51 | train loss: 0.001404 | test accuracy: 0.560897\n",
      "Step: 52 | train loss: 0.001803 | test accuracy: 0.567924\n",
      "Step: 53 | train loss: 0.006031 | test accuracy: 0.574691\n",
      "Step: 54 | train loss: 0.003282 | test accuracy: 0.581061\n",
      "Step: 55 | train loss: 0.003140 | test accuracy: 0.587202\n",
      "Step: 56 | train loss: 0.019068 | test accuracy: 0.593129\n",
      "Step: 57 | train loss: 0.008511 | test accuracy: 0.598851\n",
      "Step: 58 | train loss: 0.010633 | test accuracy: 0.604520\n",
      "Step: 59 | train loss: 0.005263 | test accuracy: 0.610139\n",
      "Step: 60 | train loss: 0.001935 | test accuracy: 0.615574\n",
      "Step: 61 | train loss: 0.006555 | test accuracy: 0.620699\n",
      "Step: 62 | train loss: 0.003698 | test accuracy: 0.625661\n",
      "Step: 63 | train loss: 0.001193 | test accuracy: 0.630599\n",
      "Step: 64 | train loss: 0.003448 | test accuracy: 0.635385\n",
      "Step: 65 | train loss: 0.001003 | test accuracy: 0.640025\n",
      "Step: 66 | train loss: 0.000891 | test accuracy: 0.644652\n",
      "Step: 67 | train loss: 0.000368 | test accuracy: 0.649142\n",
      "Step: 68 | train loss: 0.000195 | test accuracy: 0.653502\n",
      "Step: 69 | train loss: 0.000208 | test accuracy: 0.657738\n",
      "Step: 70 | train loss: 0.000639 | test accuracy: 0.661854\n",
      "Step: 71 | train loss: 0.000727 | test accuracy: 0.665856\n",
      "Step: 72 | train loss: 0.000788 | test accuracy: 0.669749\n",
      "Step: 73 | train loss: 0.000181 | test accuracy: 0.673536\n",
      "Step: 74 | train loss: 0.000234 | test accuracy: 0.677222\n",
      "Step: 75 | train loss: 0.000303 | test accuracy: 0.680702\n",
      "Step: 76 | train loss: 0.000092 | test accuracy: 0.684091\n",
      "Step: 77 | train loss: 0.000087 | test accuracy: 0.687393\n",
      "Step: 78 | train loss: 0.000042 | test accuracy: 0.690612\n",
      "Step: 79 | train loss: 0.000112 | test accuracy: 0.693750\n",
      "Step: 80 | train loss: 0.000041 | test accuracy: 0.696811\n",
      "Step: 81 | train loss: 0.000055 | test accuracy: 0.699797\n",
      "Step: 82 | train loss: 0.000019 | test accuracy: 0.702711\n",
      "Step: 83 | train loss: 0.000024 | test accuracy: 0.705556\n",
      "Step: 84 | train loss: 0.000034 | test accuracy: 0.708333\n",
      "Step: 85 | train loss: 0.000039 | test accuracy: 0.711047\n",
      "Step: 86 | train loss: 0.000017 | test accuracy: 0.713697\n",
      "Step: 87 | train loss: 0.000031 | test accuracy: 0.716288\n",
      "Step: 88 | train loss: 0.000018 | test accuracy: 0.718820\n",
      "Step: 89 | train loss: 0.000014 | test accuracy: 0.721296\n",
      "Step: 90 | train loss: 0.000022 | test accuracy: 0.723718\n",
      "Step: 91 | train loss: 0.000028 | test accuracy: 0.726087\n",
      "Step: 92 | train loss: 0.000017 | test accuracy: 0.728405\n",
      "Step: 93 | train loss: 0.000017 | test accuracy: 0.730674\n",
      "Step: 94 | train loss: 0.000014 | test accuracy: 0.732895\n",
      "Step: 95 | train loss: 0.000022 | test accuracy: 0.735069\n",
      "Step: 96 | train loss: 0.000019 | test accuracy: 0.737199\n",
      "Step: 97 | train loss: 0.000010 | test accuracy: 0.739286\n",
      "Step: 98 | train loss: 0.000018 | test accuracy: 0.741330\n",
      "Step: 99 | train loss: 0.000008 | test accuracy: 0.743333\n",
      "Step: 100 | train loss: 0.000014 | test accuracy: 0.745297\n",
      "Step: 101 | train loss: 0.000024 | test accuracy: 0.747222\n",
      "Step: 102 | train loss: 0.000011 | test accuracy: 0.749110\n",
      "Step: 103 | train loss: 0.000035 | test accuracy: 0.750962\n",
      "Step: 104 | train loss: 0.000016 | test accuracy: 0.752778\n",
      "Step: 105 | train loss: 0.000017 | test accuracy: 0.754560\n",
      "Step: 106 | train loss: 0.000010 | test accuracy: 0.756308\n",
      "Step: 107 | train loss: 0.000008 | test accuracy: 0.758025\n",
      "Step: 108 | train loss: 0.000008 | test accuracy: 0.759709\n",
      "Step: 109 | train loss: 0.000007 | test accuracy: 0.761364\n",
      "Step: 110 | train loss: 0.000020 | test accuracy: 0.762988\n",
      "Step: 111 | train loss: 0.000015 | test accuracy: 0.764583\n",
      "Step: 112 | train loss: 0.000022 | test accuracy: 0.766150\n",
      "Step: 113 | train loss: 0.000015 | test accuracy: 0.767690\n",
      "Step: 114 | train loss: 0.000010 | test accuracy: 0.769203\n",
      "Step: 115 | train loss: 0.000015 | test accuracy: 0.770690\n",
      "Step: 116 | train loss: 0.000022 | test accuracy: 0.772151\n",
      "Step: 117 | train loss: 0.000011 | test accuracy: 0.773588\n",
      "Step: 118 | train loss: 0.000017 | test accuracy: 0.775000\n",
      "Step: 119 | train loss: 0.000012 | test accuracy: 0.776389\n",
      "Step: 120 | train loss: 0.000013 | test accuracy: 0.777824\n",
      "Step: 121 | train loss: 0.000011 | test accuracy: 0.779235\n",
      "Step: 122 | train loss: 0.000011 | test accuracy: 0.780623\n",
      "Step: 123 | train loss: 0.000014 | test accuracy: 0.781989\n",
      "Step: 124 | train loss: 0.000010 | test accuracy: 0.783333\n",
      "Step: 125 | train loss: 0.000005 | test accuracy: 0.784656\n",
      "Step: 126 | train loss: 0.000004 | test accuracy: 0.785958\n",
      "Step: 127 | train loss: 0.000008 | test accuracy: 0.787240\n",
      "Step: 128 | train loss: 0.000013 | test accuracy: 0.788501\n",
      "Step: 129 | train loss: 0.000019 | test accuracy: 0.789744\n",
      "Step: 130 | train loss: 0.000009 | test accuracy: 0.790967\n",
      "Step: 131 | train loss: 0.000008 | test accuracy: 0.792172\n",
      "Step: 132 | train loss: 0.000013 | test accuracy: 0.793358\n",
      "Step: 133 | train loss: 0.000009 | test accuracy: 0.794527\n",
      "Step: 134 | train loss: 0.000006 | test accuracy: 0.795679\n",
      "Step: 135 | train loss: 0.000010 | test accuracy: 0.796814\n",
      "Step: 136 | train loss: 0.000007 | test accuracy: 0.797993\n",
      "Step: 137 | train loss: 0.000014 | test accuracy: 0.799155\n",
      "Step: 138 | train loss: 0.000007 | test accuracy: 0.800300\n",
      "Step: 139 | train loss: 0.000014 | test accuracy: 0.801429\n",
      "Step: 140 | train loss: 0.000010 | test accuracy: 0.802541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 141 | train loss: 0.000007 | test accuracy: 0.803639\n",
      "Step: 142 | train loss: 0.000007 | test accuracy: 0.804720\n",
      "Step: 143 | train loss: 0.000007 | test accuracy: 0.805787\n",
      "Step: 144 | train loss: 0.000016 | test accuracy: 0.806839\n",
      "Step: 145 | train loss: 0.000006 | test accuracy: 0.807877\n",
      "Step: 146 | train loss: 0.000007 | test accuracy: 0.808900\n",
      "Step: 147 | train loss: 0.000004 | test accuracy: 0.809910\n",
      "Step: 148 | train loss: 0.000004 | test accuracy: 0.810906\n",
      "Step: 149 | train loss: 0.000013 | test accuracy: 0.811889\n",
      "Step: 150 | train loss: 0.000005 | test accuracy: 0.812859\n",
      "Step: 151 | train loss: 0.000006 | test accuracy: 0.813816\n",
      "Step: 152 | train loss: 0.000012 | test accuracy: 0.814760\n",
      "Step: 153 | train loss: 0.000013 | test accuracy: 0.815693\n",
      "Step: 154 | train loss: 0.000004 | test accuracy: 0.816613\n",
      "Step: 155 | train loss: 0.000012 | test accuracy: 0.817521\n",
      "Step: 156 | train loss: 0.000007 | test accuracy: 0.818418\n",
      "Step: 157 | train loss: 0.000008 | test accuracy: 0.819304\n",
      "Step: 158 | train loss: 0.000011 | test accuracy: 0.820178\n",
      "Step: 159 | train loss: 0.000013 | test accuracy: 0.821042\n",
      "Step: 160 | train loss: 0.000006 | test accuracy: 0.821894\n",
      "Step: 161 | train loss: 0.000003 | test accuracy: 0.822737\n",
      "Step: 162 | train loss: 0.000008 | test accuracy: 0.823569\n",
      "Step: 163 | train loss: 0.000003 | test accuracy: 0.824390\n",
      "Step: 164 | train loss: 0.000006 | test accuracy: 0.825202\n",
      "Step: 165 | train loss: 0.000016 | test accuracy: 0.826004\n",
      "Step: 166 | train loss: 0.000004 | test accuracy: 0.826796\n",
      "Step: 167 | train loss: 0.000008 | test accuracy: 0.827579\n",
      "Step: 168 | train loss: 0.000011 | test accuracy: 0.828353\n",
      "Step: 169 | train loss: 0.000008 | test accuracy: 0.829118\n",
      "Step: 170 | train loss: 0.000010 | test accuracy: 0.829873\n",
      "Step: 171 | train loss: 0.000007 | test accuracy: 0.830620\n",
      "Step: 172 | train loss: 0.000006 | test accuracy: 0.831358\n",
      "Step: 173 | train loss: 0.000004 | test accuracy: 0.832088\n",
      "Step: 174 | train loss: 0.000005 | test accuracy: 0.832810\n",
      "Step: 175 | train loss: 0.000007 | test accuracy: 0.833523\n",
      "Step: 176 | train loss: 0.000007 | test accuracy: 0.834228\n",
      "Step: 177 | train loss: 0.000006 | test accuracy: 0.834925\n",
      "Step: 178 | train loss: 0.000008 | test accuracy: 0.835615\n",
      "Step: 179 | train loss: 0.000003 | test accuracy: 0.836296\n",
      "Step: 180 | train loss: 0.000011 | test accuracy: 0.836971\n",
      "Step: 181 | train loss: 0.000005 | test accuracy: 0.837637\n",
      "Step: 182 | train loss: 0.000007 | test accuracy: 0.838297\n",
      "Step: 183 | train loss: 0.000004 | test accuracy: 0.838949\n",
      "Step: 184 | train loss: 0.000007 | test accuracy: 0.839595\n",
      "Step: 185 | train loss: 0.000008 | test accuracy: 0.840233\n",
      "Step: 186 | train loss: 0.000010 | test accuracy: 0.840865\n",
      "Step: 187 | train loss: 0.000007 | test accuracy: 0.841489\n",
      "Step: 188 | train loss: 0.000004 | test accuracy: 0.842108\n",
      "Step: 189 | train loss: 0.000008 | test accuracy: 0.842719\n",
      "Step: 190 | train loss: 0.000004 | test accuracy: 0.843325\n",
      "Step: 191 | train loss: 0.000006 | test accuracy: 0.843924\n",
      "Step: 192 | train loss: 0.000010 | test accuracy: 0.844516\n",
      "Step: 193 | train loss: 0.000009 | test accuracy: 0.845103\n",
      "Step: 194 | train loss: 0.000004 | test accuracy: 0.845684\n",
      "Step: 195 | train loss: 0.000006 | test accuracy: 0.846259\n",
      "Step: 196 | train loss: 0.000002 | test accuracy: 0.846827\n",
      "Step: 197 | train loss: 0.000005 | test accuracy: 0.847391\n",
      "Step: 198 | train loss: 0.000008 | test accuracy: 0.847948\n",
      "Step: 199 | train loss: 0.000002 | test accuracy: 0.848500\n",
      "Step: 200 | train loss: 0.000002 | test accuracy: 0.849046\n",
      "Step: 201 | train loss: 0.000009 | test accuracy: 0.849587\n",
      "Step: 202 | train loss: 0.000009 | test accuracy: 0.850123\n",
      "Step: 203 | train loss: 0.000009 | test accuracy: 0.850654\n",
      "Step: 204 | train loss: 0.000004 | test accuracy: 0.851179\n",
      "Step: 205 | train loss: 0.000012 | test accuracy: 0.851699\n",
      "Step: 206 | train loss: 0.000005 | test accuracy: 0.852214\n",
      "Step: 207 | train loss: 0.000007 | test accuracy: 0.852724\n",
      "Step: 208 | train loss: 0.000008 | test accuracy: 0.853230\n",
      "Step: 209 | train loss: 0.000004 | test accuracy: 0.853730\n",
      "Step: 210 | train loss: 0.000005 | test accuracy: 0.854226\n",
      "Step: 211 | train loss: 0.000003 | test accuracy: 0.854717\n",
      "Step: 212 | train loss: 0.000008 | test accuracy: 0.855203\n",
      "Step: 213 | train loss: 0.000008 | test accuracy: 0.855685\n",
      "Step: 214 | train loss: 0.000004 | test accuracy: 0.856163\n",
      "Step: 215 | train loss: 0.000014 | test accuracy: 0.856636\n",
      "Step: 216 | train loss: 0.000004 | test accuracy: 0.857104\n",
      "Step: 217 | train loss: 0.000008 | test accuracy: 0.857569\n",
      "Step: 218 | train loss: 0.000007 | test accuracy: 0.858029\n",
      "Step: 219 | train loss: 0.000005 | test accuracy: 0.858485\n",
      "Step: 220 | train loss: 0.000006 | test accuracy: 0.858937\n",
      "Step: 221 | train loss: 0.000011 | test accuracy: 0.859384\n",
      "Step: 222 | train loss: 0.000006 | test accuracy: 0.859828\n",
      "Step: 223 | train loss: 0.000002 | test accuracy: 0.860268\n",
      "Step: 224 | train loss: 0.000003 | test accuracy: 0.860704\n",
      "Step: 225 | train loss: 0.000005 | test accuracy: 0.861136\n",
      "Step: 226 | train loss: 0.000003 | test accuracy: 0.861564\n",
      "Step: 227 | train loss: 0.000003 | test accuracy: 0.861988\n",
      "Step: 228 | train loss: 0.000009 | test accuracy: 0.862409\n",
      "Step: 229 | train loss: 0.000007 | test accuracy: 0.862826\n",
      "Step: 230 | train loss: 0.000006 | test accuracy: 0.863240\n",
      "Step: 231 | train loss: 0.000004 | test accuracy: 0.863649\n",
      "Step: 232 | train loss: 0.000001 | test accuracy: 0.864056\n",
      "Step: 233 | train loss: 0.000008 | test accuracy: 0.864459\n",
      "Step: 234 | train loss: 0.000004 | test accuracy: 0.864858\n",
      "Step: 235 | train loss: 0.000003 | test accuracy: 0.865254\n",
      "Step: 236 | train loss: 0.000006 | test accuracy: 0.865647\n",
      "Step: 237 | train loss: 0.000003 | test accuracy: 0.866036\n",
      "Step: 238 | train loss: 0.000008 | test accuracy: 0.866423\n",
      "Step: 239 | train loss: 0.000003 | test accuracy: 0.866806\n",
      "Step: 240 | train loss: 0.000006 | test accuracy: 0.867185\n",
      "Step: 241 | train loss: 0.000005 | test accuracy: 0.867562\n",
      "Step: 242 | train loss: 0.000006 | test accuracy: 0.867936\n",
      "Step: 243 | train loss: 0.000005 | test accuracy: 0.868306\n",
      "Step: 244 | train loss: 0.000002 | test accuracy: 0.868673\n",
      "Step: 245 | train loss: 0.000004 | test accuracy: 0.869038\n",
      "Step: 246 | train loss: 0.000006 | test accuracy: 0.869399\n",
      "Step: 247 | train loss: 0.000004 | test accuracy: 0.869758\n",
      "Step: 248 | train loss: 0.000004 | test accuracy: 0.870114\n",
      "Step: 249 | train loss: 0.000007 | test accuracy: 0.870467\n",
      "Step: 250 | train loss: 0.000003 | test accuracy: 0.870817\n",
      "Step: 251 | train loss: 0.000004 | test accuracy: 0.871164\n",
      "Step: 252 | train loss: 0.000007 | test accuracy: 0.871509\n",
      "Step: 253 | train loss: 0.000009 | test accuracy: 0.871850\n",
      "Step: 254 | train loss: 0.000005 | test accuracy: 0.872190\n",
      "Step: 255 | train loss: 0.000004 | test accuracy: 0.872526\n",
      "Step: 256 | train loss: 0.000003 | test accuracy: 0.872860\n",
      "Step: 257 | train loss: 0.000002 | test accuracy: 0.873191\n",
      "Step: 258 | train loss: 0.000005 | test accuracy: 0.873520\n",
      "Step: 259 | train loss: 0.000004 | test accuracy: 0.873846\n",
      "Step: 260 | train loss: 0.000008 | test accuracy: 0.874170\n",
      "Step: 261 | train loss: 0.000006 | test accuracy: 0.874491\n",
      "Step: 262 | train loss: 0.000003 | test accuracy: 0.874810\n",
      "Step: 263 | train loss: 0.000006 | test accuracy: 0.875126\n",
      "Step: 264 | train loss: 0.000003 | test accuracy: 0.875440\n",
      "Step: 265 | train loss: 0.000005 | test accuracy: 0.875752\n",
      "Step: 266 | train loss: 0.000005 | test accuracy: 0.876061\n",
      "Step: 267 | train loss: 0.000006 | test accuracy: 0.876368\n",
      "Step: 268 | train loss: 0.000007 | test accuracy: 0.876673\n",
      "Step: 269 | train loss: 0.000010 | test accuracy: 0.876975\n",
      "Step: 270 | train loss: 0.000005 | test accuracy: 0.877276\n",
      "Step: 271 | train loss: 0.000005 | test accuracy: 0.877574\n",
      "Step: 272 | train loss: 0.000005 | test accuracy: 0.877869\n",
      "Step: 273 | train loss: 0.000003 | test accuracy: 0.878163\n",
      "Step: 274 | train loss: 0.000003 | test accuracy: 0.878455\n",
      "Step: 275 | train loss: 0.000006 | test accuracy: 0.878744\n",
      "Step: 276 | train loss: 0.000005 | test accuracy: 0.879031\n",
      "Step: 277 | train loss: 0.000002 | test accuracy: 0.879317\n",
      "Step: 278 | train loss: 0.000006 | test accuracy: 0.879600\n",
      "Step: 279 | train loss: 0.000007 | test accuracy: 0.879881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 280 | train loss: 0.000002 | test accuracy: 0.880160\n",
      "Step: 281 | train loss: 0.000003 | test accuracy: 0.880437\n",
      "Step: 282 | train loss: 0.000005 | test accuracy: 0.880713\n",
      "Step: 283 | train loss: 0.000006 | test accuracy: 0.880986\n",
      "Step: 284 | train loss: 0.000004 | test accuracy: 0.881257\n",
      "Step: 285 | train loss: 0.000004 | test accuracy: 0.881527\n",
      "Step: 286 | train loss: 0.000002 | test accuracy: 0.881794\n",
      "Step: 287 | train loss: 0.000003 | test accuracy: 0.882060\n",
      "Step: 288 | train loss: 0.000003 | test accuracy: 0.882324\n",
      "Step: 289 | train loss: 0.000002 | test accuracy: 0.882586\n",
      "Step: 290 | train loss: 0.000003 | test accuracy: 0.882847\n",
      "Step: 291 | train loss: 0.000003 | test accuracy: 0.883105\n",
      "Step: 292 | train loss: 0.000004 | test accuracy: 0.883362\n",
      "Step: 293 | train loss: 0.000001 | test accuracy: 0.883617\n",
      "Step: 294 | train loss: 0.000002 | test accuracy: 0.883870\n",
      "Step: 295 | train loss: 0.000004 | test accuracy: 0.884122\n",
      "Step: 296 | train loss: 0.000002 | test accuracy: 0.884372\n",
      "Step: 297 | train loss: 0.000005 | test accuracy: 0.884620\n",
      "Step: 298 | train loss: 0.000006 | test accuracy: 0.884866\n",
      "Step: 299 | train loss: 0.000001 | test accuracy: 0.885111\n",
      "Step: 300 | train loss: 0.000005 | test accuracy: 0.885354\n",
      "Step: 301 | train loss: 0.000002 | test accuracy: 0.885596\n",
      "Step: 302 | train loss: 0.000003 | test accuracy: 0.885836\n",
      "Step: 303 | train loss: 0.000003 | test accuracy: 0.886075\n",
      "Step: 304 | train loss: 0.000006 | test accuracy: 0.886311\n",
      "Step: 305 | train loss: 0.000002 | test accuracy: 0.886547\n",
      "Step: 306 | train loss: 0.000003 | test accuracy: 0.886781\n",
      "Step: 307 | train loss: 0.000003 | test accuracy: 0.887013\n",
      "Step: 308 | train loss: 0.000003 | test accuracy: 0.887244\n",
      "Step: 309 | train loss: 0.000005 | test accuracy: 0.887473\n",
      "Step: 310 | train loss: 0.000004 | test accuracy: 0.887701\n",
      "Step: 311 | train loss: 0.000005 | test accuracy: 0.887927\n",
      "Step: 312 | train loss: 0.000005 | test accuracy: 0.888152\n",
      "Step: 313 | train loss: 0.000007 | test accuracy: 0.888376\n",
      "Step: 314 | train loss: 0.000005 | test accuracy: 0.888598\n",
      "Step: 315 | train loss: 0.000005 | test accuracy: 0.888819\n",
      "Step: 316 | train loss: 0.000003 | test accuracy: 0.889038\n",
      "Step: 317 | train loss: 0.000003 | test accuracy: 0.889256\n",
      "Step: 318 | train loss: 0.000004 | test accuracy: 0.889472\n",
      "Step: 319 | train loss: 0.000003 | test accuracy: 0.889687\n",
      "Step: 320 | train loss: 0.000002 | test accuracy: 0.889901\n",
      "Step: 321 | train loss: 0.000006 | test accuracy: 0.890114\n",
      "Step: 322 | train loss: 0.000004 | test accuracy: 0.890325\n",
      "Step: 323 | train loss: 0.000003 | test accuracy: 0.890535\n",
      "Step: 324 | train loss: 0.000002 | test accuracy: 0.890744\n",
      "Step: 325 | train loss: 0.000004 | test accuracy: 0.890951\n",
      "Step: 326 | train loss: 0.000002 | test accuracy: 0.891157\n",
      "Step: 327 | train loss: 0.000005 | test accuracy: 0.891362\n",
      "Step: 328 | train loss: 0.000004 | test accuracy: 0.891565\n",
      "Step: 329 | train loss: 0.000004 | test accuracy: 0.891768\n",
      "Step: 330 | train loss: 0.000001 | test accuracy: 0.891969\n",
      "Step: 331 | train loss: 0.000005 | test accuracy: 0.892169\n",
      "Step: 332 | train loss: 0.000003 | test accuracy: 0.892367\n",
      "Step: 333 | train loss: 0.000006 | test accuracy: 0.892565\n",
      "Step: 334 | train loss: 0.000006 | test accuracy: 0.892761\n",
      "Step: 335 | train loss: 0.000005 | test accuracy: 0.892956\n",
      "Step: 336 | train loss: 0.000006 | test accuracy: 0.893150\n",
      "Step: 337 | train loss: 0.000003 | test accuracy: 0.893343\n",
      "Step: 338 | train loss: 0.000004 | test accuracy: 0.893535\n",
      "Step: 339 | train loss: 0.000004 | test accuracy: 0.893726\n",
      "Step: 340 | train loss: 0.000004 | test accuracy: 0.893915\n",
      "Step: 341 | train loss: 0.000005 | test accuracy: 0.894103\n",
      "Step: 342 | train loss: 0.000005 | test accuracy: 0.894291\n",
      "Step: 343 | train loss: 0.000004 | test accuracy: 0.894477\n",
      "Step: 344 | train loss: 0.000005 | test accuracy: 0.894662\n",
      "Step: 345 | train loss: 0.000004 | test accuracy: 0.894846\n",
      "Step: 346 | train loss: 0.000002 | test accuracy: 0.895029\n",
      "Step: 347 | train loss: 0.000003 | test accuracy: 0.895211\n",
      "Step: 348 | train loss: 0.000001 | test accuracy: 0.895392\n",
      "Step: 349 | train loss: 0.000002 | test accuracy: 0.895571\n",
      "Step: 350 | train loss: 0.000003 | test accuracy: 0.895750\n",
      "Step: 351 | train loss: 0.000004 | test accuracy: 0.895928\n",
      "Step: 352 | train loss: 0.000005 | test accuracy: 0.896105\n",
      "Step: 353 | train loss: 0.000003 | test accuracy: 0.896281\n",
      "Step: 354 | train loss: 0.000005 | test accuracy: 0.896455\n",
      "Step: 355 | train loss: 0.000004 | test accuracy: 0.896629\n",
      "Step: 356 | train loss: 0.000002 | test accuracy: 0.896802\n",
      "Step: 357 | train loss: 0.000001 | test accuracy: 0.896974\n",
      "Step: 358 | train loss: 0.000004 | test accuracy: 0.897145\n",
      "Step: 359 | train loss: 0.000003 | test accuracy: 0.897315\n",
      "Step: 360 | train loss: 0.000003 | test accuracy: 0.897484\n",
      "Step: 361 | train loss: 0.000002 | test accuracy: 0.897652\n",
      "Step: 362 | train loss: 0.000002 | test accuracy: 0.897819\n",
      "Step: 363 | train loss: 0.000004 | test accuracy: 0.897985\n",
      "Step: 364 | train loss: 0.000003 | test accuracy: 0.898151\n",
      "Step: 365 | train loss: 0.000004 | test accuracy: 0.898315\n",
      "Step: 366 | train loss: 0.000002 | test accuracy: 0.898479\n",
      "Step: 367 | train loss: 0.000005 | test accuracy: 0.898641\n",
      "Step: 368 | train loss: 0.000005 | test accuracy: 0.898803\n",
      "Step: 369 | train loss: 0.000001 | test accuracy: 0.898964\n",
      "Step: 370 | train loss: 0.000003 | test accuracy: 0.899124\n",
      "Step: 371 | train loss: 0.000002 | test accuracy: 0.899283\n",
      "Step: 372 | train loss: 0.000005 | test accuracy: 0.899441\n",
      "Step: 373 | train loss: 0.000003 | test accuracy: 0.899599\n",
      "Step: 374 | train loss: 0.000003 | test accuracy: 0.899756\n",
      "Step: 375 | train loss: 0.000003 | test accuracy: 0.899911\n",
      "Step: 376 | train loss: 0.000004 | test accuracy: 0.900066\n",
      "Step: 377 | train loss: 0.000003 | test accuracy: 0.900220\n",
      "Step: 378 | train loss: 0.000004 | test accuracy: 0.900374\n",
      "Step: 379 | train loss: 0.000004 | test accuracy: 0.900526\n",
      "Step: 380 | train loss: 0.000002 | test accuracy: 0.900678\n",
      "Step: 381 | train loss: 0.000002 | test accuracy: 0.900829\n",
      "Step: 382 | train loss: 0.000005 | test accuracy: 0.900979\n",
      "Step: 383 | train loss: 0.000004 | test accuracy: 0.901128\n",
      "Step: 384 | train loss: 0.000002 | test accuracy: 0.901277\n",
      "Step: 385 | train loss: 0.000003 | test accuracy: 0.901425\n",
      "Step: 386 | train loss: 0.000002 | test accuracy: 0.901572\n",
      "Step: 387 | train loss: 0.000003 | test accuracy: 0.901718\n",
      "Step: 388 | train loss: 0.000001 | test accuracy: 0.901864\n",
      "Step: 389 | train loss: 0.000002 | test accuracy: 0.902009\n",
      "Step: 390 | train loss: 0.000005 | test accuracy: 0.902153\n",
      "Step: 391 | train loss: 0.000003 | test accuracy: 0.902296\n",
      "Step: 392 | train loss: 0.000003 | test accuracy: 0.902439\n",
      "Step: 393 | train loss: 0.000001 | test accuracy: 0.902580\n",
      "Step: 394 | train loss: 0.000003 | test accuracy: 0.902722\n",
      "Step: 395 | train loss: 0.000002 | test accuracy: 0.902862\n",
      "Step: 396 | train loss: 0.000005 | test accuracy: 0.903002\n",
      "Step: 397 | train loss: 0.000005 | test accuracy: 0.903141\n",
      "Step: 398 | train loss: 0.000005 | test accuracy: 0.903279\n",
      "Step: 399 | train loss: 0.000003 | test accuracy: 0.903417\n",
      "Step: 400 | train loss: 0.000002 | test accuracy: 0.903554\n",
      "Step: 401 | train loss: 0.000002 | test accuracy: 0.903690\n",
      "Step: 402 | train loss: 0.000006 | test accuracy: 0.903825\n",
      "Step: 403 | train loss: 0.000003 | test accuracy: 0.903960\n",
      "Step: 404 | train loss: 0.000003 | test accuracy: 0.904095\n",
      "Step: 405 | train loss: 0.000005 | test accuracy: 0.904228\n",
      "Step: 406 | train loss: 0.000005 | test accuracy: 0.904361\n",
      "Step: 407 | train loss: 0.000004 | test accuracy: 0.904493\n",
      "Step: 408 | train loss: 0.000004 | test accuracy: 0.904625\n",
      "Step: 409 | train loss: 0.000002 | test accuracy: 0.904756\n",
      "Step: 410 | train loss: 0.000003 | test accuracy: 0.904886\n",
      "Step: 411 | train loss: 0.000003 | test accuracy: 0.905016\n",
      "Step: 412 | train loss: 0.000002 | test accuracy: 0.905145\n",
      "Step: 413 | train loss: 0.000004 | test accuracy: 0.905274\n",
      "Step: 414 | train loss: 0.000002 | test accuracy: 0.905402\n",
      "Step: 415 | train loss: 0.000003 | test accuracy: 0.905529\n",
      "Step: 416 | train loss: 0.000003 | test accuracy: 0.905656\n",
      "Step: 417 | train loss: 0.000002 | test accuracy: 0.905782\n",
      "Step: 418 | train loss: 0.000005 | test accuracy: 0.905907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 419 | train loss: 0.000003 | test accuracy: 0.906032\n",
      "Step: 420 | train loss: 0.000004 | test accuracy: 0.906156\n",
      "Step: 421 | train loss: 0.000006 | test accuracy: 0.906280\n",
      "Step: 422 | train loss: 0.000003 | test accuracy: 0.906403\n",
      "Step: 423 | train loss: 0.000002 | test accuracy: 0.906525\n",
      "Step: 424 | train loss: 0.000002 | test accuracy: 0.906647\n",
      "Step: 425 | train loss: 0.000004 | test accuracy: 0.906768\n",
      "Step: 426 | train loss: 0.000004 | test accuracy: 0.906889\n",
      "Step: 427 | train loss: 0.000003 | test accuracy: 0.907009\n",
      "Step: 428 | train loss: 0.000003 | test accuracy: 0.907129\n",
      "Step: 429 | train loss: 0.000001 | test accuracy: 0.907248\n",
      "Step: 430 | train loss: 0.000003 | test accuracy: 0.907367\n",
      "Step: 431 | train loss: 0.000003 | test accuracy: 0.907485\n",
      "Step: 432 | train loss: 0.000001 | test accuracy: 0.907602\n",
      "Step: 433 | train loss: 0.000003 | test accuracy: 0.907719\n",
      "Step: 434 | train loss: 0.000002 | test accuracy: 0.907835\n",
      "Step: 435 | train loss: 0.000002 | test accuracy: 0.907951\n",
      "Step: 436 | train loss: 0.000003 | test accuracy: 0.908066\n",
      "Step: 437 | train loss: 0.000003 | test accuracy: 0.908181\n",
      "Step: 438 | train loss: 0.000001 | test accuracy: 0.908295\n",
      "Step: 439 | train loss: 0.000002 | test accuracy: 0.908409\n",
      "Step: 440 | train loss: 0.000003 | test accuracy: 0.908522\n",
      "Step: 441 | train loss: 0.000002 | test accuracy: 0.908635\n",
      "Step: 442 | train loss: 0.000002 | test accuracy: 0.908747\n",
      "Step: 443 | train loss: 0.000002 | test accuracy: 0.908859\n",
      "Step: 444 | train loss: 0.000003 | test accuracy: 0.908970\n",
      "Step: 445 | train loss: 0.000003 | test accuracy: 0.909081\n",
      "Step: 446 | train loss: 0.000002 | test accuracy: 0.909191\n",
      "Step: 447 | train loss: 0.000002 | test accuracy: 0.909301\n",
      "Step: 448 | train loss: 0.000004 | test accuracy: 0.909410\n",
      "Step: 449 | train loss: 0.000003 | test accuracy: 0.909519\n",
      "Step: 450 | train loss: 0.000004 | test accuracy: 0.909627\n",
      "Step: 451 | train loss: 0.000003 | test accuracy: 0.909734\n",
      "Step: 452 | train loss: 0.000004 | test accuracy: 0.909842\n",
      "Step: 453 | train loss: 0.000003 | test accuracy: 0.909949\n",
      "Step: 454 | train loss: 0.000002 | test accuracy: 0.910055\n",
      "Step: 455 | train loss: 0.000003 | test accuracy: 0.910161\n",
      "Step: 456 | train loss: 0.000004 | test accuracy: 0.910266\n",
      "Step: 457 | train loss: 0.000003 | test accuracy: 0.910371\n",
      "Step: 458 | train loss: 0.000002 | test accuracy: 0.910476\n",
      "Step: 459 | train loss: 0.000003 | test accuracy: 0.910580\n",
      "Step: 460 | train loss: 0.000002 | test accuracy: 0.910683\n",
      "Step: 461 | train loss: 0.000003 | test accuracy: 0.910786\n",
      "Step: 462 | train loss: 0.000003 | test accuracy: 0.910889\n",
      "Step: 463 | train loss: 0.000003 | test accuracy: 0.910991\n",
      "Step: 464 | train loss: 0.000004 | test accuracy: 0.911093\n",
      "Step: 465 | train loss: 0.000004 | test accuracy: 0.911195\n",
      "Step: 466 | train loss: 0.000002 | test accuracy: 0.911295\n",
      "Step: 467 | train loss: 0.000001 | test accuracy: 0.911396\n",
      "Step: 468 | train loss: 0.000001 | test accuracy: 0.911496\n",
      "Step: 469 | train loss: 0.000001 | test accuracy: 0.911596\n",
      "Step: 470 | train loss: 0.000002 | test accuracy: 0.911695\n",
      "Step: 471 | train loss: 0.000002 | test accuracy: 0.911794\n",
      "Step: 472 | train loss: 0.000003 | test accuracy: 0.911892\n",
      "Step: 473 | train loss: 0.000002 | test accuracy: 0.911990\n",
      "Step: 474 | train loss: 0.000002 | test accuracy: 0.912088\n",
      "Step: 475 | train loss: 0.000003 | test accuracy: 0.912185\n",
      "Step: 476 | train loss: 0.000002 | test accuracy: 0.912282\n",
      "Step: 477 | train loss: 0.000003 | test accuracy: 0.912378\n",
      "Step: 478 | train loss: 0.000003 | test accuracy: 0.912474\n",
      "Step: 479 | train loss: 0.000003 | test accuracy: 0.912569\n",
      "Step: 480 | train loss: 0.000003 | test accuracy: 0.912665\n",
      "Step: 481 | train loss: 0.000002 | test accuracy: 0.912759\n",
      "Step: 482 | train loss: 0.000002 | test accuracy: 0.912854\n",
      "Step: 483 | train loss: 0.000002 | test accuracy: 0.912948\n",
      "Step: 484 | train loss: 0.000004 | test accuracy: 0.913041\n",
      "Step: 485 | train loss: 0.000003 | test accuracy: 0.913134\n",
      "Step: 486 | train loss: 0.000002 | test accuracy: 0.913227\n",
      "Step: 487 | train loss: 0.000003 | test accuracy: 0.913320\n",
      "Step: 488 | train loss: 0.000003 | test accuracy: 0.913412\n",
      "Step: 489 | train loss: 0.000003 | test accuracy: 0.913503\n",
      "Step: 490 | train loss: 0.000002 | test accuracy: 0.913595\n",
      "Step: 491 | train loss: 0.000003 | test accuracy: 0.913686\n",
      "Step: 492 | train loss: 0.000001 | test accuracy: 0.913776\n",
      "Step: 493 | train loss: 0.000003 | test accuracy: 0.913866\n",
      "Step: 494 | train loss: 0.000002 | test accuracy: 0.913956\n",
      "Step: 495 | train loss: 0.000002 | test accuracy: 0.914046\n",
      "Step: 496 | train loss: 0.000003 | test accuracy: 0.914135\n",
      "Step: 497 | train loss: 0.000003 | test accuracy: 0.914224\n",
      "Step: 498 | train loss: 0.000003 | test accuracy: 0.914312\n",
      "Step: 499 | train loss: 0.000002 | test accuracy: 0.914400\n",
      "Step: 500 | train loss: 0.000002 | test accuracy: 0.914488\n",
      "Step: 501 | train loss: 0.000002 | test accuracy: 0.914575\n",
      "Step: 502 | train loss: 0.000001 | test accuracy: 0.914662\n",
      "Step: 503 | train loss: 0.000002 | test accuracy: 0.914749\n",
      "Step: 504 | train loss: 0.000002 | test accuracy: 0.914835\n",
      "Step: 505 | train loss: 0.000002 | test accuracy: 0.914921\n",
      "Step: 506 | train loss: 0.000004 | test accuracy: 0.915007\n",
      "Step: 507 | train loss: 0.000002 | test accuracy: 0.915092\n",
      "Step: 508 | train loss: 0.000002 | test accuracy: 0.915177\n",
      "Step: 509 | train loss: 0.000003 | test accuracy: 0.915261\n",
      "Step: 510 | train loss: 0.000002 | test accuracy: 0.915346\n",
      "Step: 511 | train loss: 0.000002 | test accuracy: 0.915430\n",
      "Step: 512 | train loss: 0.000002 | test accuracy: 0.915513\n",
      "Step: 513 | train loss: 0.000002 | test accuracy: 0.915597\n",
      "Step: 514 | train loss: 0.000003 | test accuracy: 0.915680\n",
      "Step: 515 | train loss: 0.000002 | test accuracy: 0.915762\n",
      "Step: 516 | train loss: 0.000002 | test accuracy: 0.915845\n",
      "Step: 517 | train loss: 0.000002 | test accuracy: 0.915927\n",
      "Step: 518 | train loss: 0.000003 | test accuracy: 0.916008\n",
      "Step: 519 | train loss: 0.000002 | test accuracy: 0.916090\n",
      "Step: 520 | train loss: 0.000001 | test accuracy: 0.916171\n",
      "Step: 521 | train loss: 0.000003 | test accuracy: 0.916252\n",
      "Step: 522 | train loss: 0.000003 | test accuracy: 0.916332\n",
      "Step: 523 | train loss: 0.000002 | test accuracy: 0.916412\n",
      "Step: 524 | train loss: 0.000002 | test accuracy: 0.916492\n",
      "Step: 525 | train loss: 0.000002 | test accuracy: 0.916572\n",
      "Step: 526 | train loss: 0.000003 | test accuracy: 0.916651\n",
      "Step: 527 | train loss: 0.000002 | test accuracy: 0.916730\n",
      "Step: 528 | train loss: 0.000001 | test accuracy: 0.916808\n",
      "Step: 529 | train loss: 0.000002 | test accuracy: 0.916887\n",
      "Step: 530 | train loss: 0.000003 | test accuracy: 0.916965\n",
      "Step: 531 | train loss: 0.000004 | test accuracy: 0.917043\n",
      "Step: 532 | train loss: 0.000002 | test accuracy: 0.917120\n",
      "Step: 533 | train loss: 0.000001 | test accuracy: 0.917197\n",
      "Step: 534 | train loss: 0.000003 | test accuracy: 0.917274\n",
      "Step: 535 | train loss: 0.000002 | test accuracy: 0.917351\n",
      "Step: 536 | train loss: 0.000003 | test accuracy: 0.917427\n",
      "Step: 537 | train loss: 0.000002 | test accuracy: 0.917503\n",
      "Step: 538 | train loss: 0.000002 | test accuracy: 0.917579\n",
      "Step: 539 | train loss: 0.000002 | test accuracy: 0.917654\n",
      "Step: 540 | train loss: 0.000002 | test accuracy: 0.917729\n",
      "Step: 541 | train loss: 0.000001 | test accuracy: 0.917804\n",
      "Step: 542 | train loss: 0.000002 | test accuracy: 0.917879\n",
      "Step: 543 | train loss: 0.000001 | test accuracy: 0.917953\n",
      "Step: 544 | train loss: 0.000002 | test accuracy: 0.918028\n",
      "Step: 545 | train loss: 0.000001 | test accuracy: 0.918101\n",
      "Step: 546 | train loss: 0.000002 | test accuracy: 0.918175\n",
      "Step: 547 | train loss: 0.000002 | test accuracy: 0.918248\n",
      "Step: 548 | train loss: 0.000002 | test accuracy: 0.918321\n",
      "Step: 549 | train loss: 0.000002 | test accuracy: 0.918394\n",
      "Step: 550 | train loss: 0.000001 | test accuracy: 0.918466\n",
      "Step: 551 | train loss: 0.000002 | test accuracy: 0.918539\n",
      "Step: 552 | train loss: 0.000002 | test accuracy: 0.918611\n",
      "Step: 553 | train loss: 0.000001 | test accuracy: 0.918682\n",
      "Step: 554 | train loss: 0.000002 | test accuracy: 0.918754\n",
      "Step: 555 | train loss: 0.000003 | test accuracy: 0.918825\n",
      "Step: 556 | train loss: 0.000003 | test accuracy: 0.918896\n",
      "Step: 557 | train loss: 0.000003 | test accuracy: 0.918967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 558 | train loss: 0.000003 | test accuracy: 0.919037\n",
      "Step: 559 | train loss: 0.000002 | test accuracy: 0.919107\n",
      "Step: 560 | train loss: 0.000001 | test accuracy: 0.919177\n",
      "Step: 561 | train loss: 0.000002 | test accuracy: 0.919247\n",
      "Step: 562 | train loss: 0.000001 | test accuracy: 0.919316\n",
      "Step: 563 | train loss: 0.000001 | test accuracy: 0.919385\n",
      "Step: 564 | train loss: 0.000002 | test accuracy: 0.919454\n",
      "Step: 565 | train loss: 0.000003 | test accuracy: 0.919523\n",
      "Step: 566 | train loss: 0.000002 | test accuracy: 0.919591\n",
      "Step: 567 | train loss: 0.000004 | test accuracy: 0.919660\n",
      "Step: 568 | train loss: 0.000002 | test accuracy: 0.919728\n",
      "Step: 569 | train loss: 0.000002 | test accuracy: 0.919795\n",
      "Step: 570 | train loss: 0.000001 | test accuracy: 0.919863\n",
      "Step: 571 | train loss: 0.000003 | test accuracy: 0.919930\n",
      "Step: 572 | train loss: 0.000002 | test accuracy: 0.919997\n",
      "Step: 573 | train loss: 0.000001 | test accuracy: 0.920064\n",
      "Step: 574 | train loss: 0.000002 | test accuracy: 0.920130\n",
      "Step: 575 | train loss: 0.000002 | test accuracy: 0.920197\n",
      "Step: 576 | train loss: 0.000002 | test accuracy: 0.920263\n",
      "Step: 577 | train loss: 0.000002 | test accuracy: 0.920329\n",
      "Step: 578 | train loss: 0.000002 | test accuracy: 0.920394\n",
      "Step: 579 | train loss: 0.000002 | test accuracy: 0.920460\n",
      "Step: 580 | train loss: 0.000002 | test accuracy: 0.920525\n",
      "Step: 581 | train loss: 0.000002 | test accuracy: 0.920590\n",
      "Step: 582 | train loss: 0.000001 | test accuracy: 0.920655\n",
      "Step: 583 | train loss: 0.000002 | test accuracy: 0.920719\n",
      "Step: 584 | train loss: 0.000003 | test accuracy: 0.920783\n",
      "Step: 585 | train loss: 0.000002 | test accuracy: 0.920848\n",
      "Step: 586 | train loss: 0.000002 | test accuracy: 0.920911\n",
      "Step: 587 | train loss: 0.000002 | test accuracy: 0.920975\n",
      "Step: 588 | train loss: 0.000001 | test accuracy: 0.921039\n",
      "Step: 589 | train loss: 0.000002 | test accuracy: 0.921102\n",
      "Step: 590 | train loss: 0.000002 | test accuracy: 0.921165\n",
      "Step: 591 | train loss: 0.000002 | test accuracy: 0.921227\n",
      "Step: 592 | train loss: 0.000003 | test accuracy: 0.921290\n",
      "Step: 593 | train loss: 0.000002 | test accuracy: 0.921352\n",
      "Step: 594 | train loss: 0.000002 | test accuracy: 0.921415\n",
      "Step: 595 | train loss: 0.000002 | test accuracy: 0.921476\n",
      "Step: 596 | train loss: 0.000002 | test accuracy: 0.921538\n",
      "Step: 597 | train loss: 0.000002 | test accuracy: 0.921600\n",
      "Step: 598 | train loss: 0.000002 | test accuracy: 0.921661\n",
      "Step: 599 | train loss: 0.000002 | test accuracy: 0.921722\n",
      "Step: 600 | train loss: 0.000002 | test accuracy: 0.921783\n",
      "Step: 601 | train loss: 0.000002 | test accuracy: 0.921844\n",
      "Step: 602 | train loss: 0.000002 | test accuracy: 0.921904\n",
      "Step: 603 | train loss: 0.000002 | test accuracy: 0.921965\n",
      "Step: 604 | train loss: 0.000003 | test accuracy: 0.922025\n",
      "Step: 605 | train loss: 0.000002 | test accuracy: 0.922085\n",
      "Step: 606 | train loss: 0.000001 | test accuracy: 0.922144\n",
      "Step: 607 | train loss: 0.000002 | test accuracy: 0.922204\n",
      "Step: 608 | train loss: 0.000002 | test accuracy: 0.922263\n",
      "Step: 609 | train loss: 0.000001 | test accuracy: 0.922322\n",
      "Step: 610 | train loss: 0.000002 | test accuracy: 0.922381\n",
      "Step: 611 | train loss: 0.000002 | test accuracy: 0.922440\n",
      "Step: 612 | train loss: 0.000003 | test accuracy: 0.922499\n",
      "Step: 613 | train loss: 0.000002 | test accuracy: 0.922557\n",
      "Step: 614 | train loss: 0.000001 | test accuracy: 0.922615\n",
      "Step: 615 | train loss: 0.000001 | test accuracy: 0.922673\n",
      "Step: 616 | train loss: 0.000002 | test accuracy: 0.922731\n",
      "Step: 617 | train loss: 0.000002 | test accuracy: 0.922789\n",
      "Step: 618 | train loss: 0.000001 | test accuracy: 0.922846\n",
      "Step: 619 | train loss: 0.000002 | test accuracy: 0.922903\n",
      "Step: 620 | train loss: 0.000002 | test accuracy: 0.922960\n",
      "Step: 621 | train loss: 0.000001 | test accuracy: 0.923017\n",
      "Step: 622 | train loss: 0.000003 | test accuracy: 0.923074\n",
      "Step: 623 | train loss: 0.000001 | test accuracy: 0.923130\n",
      "Step: 624 | train loss: 0.000002 | test accuracy: 0.923187\n",
      "Step: 625 | train loss: 0.000001 | test accuracy: 0.923243\n",
      "Step: 626 | train loss: 0.000002 | test accuracy: 0.923299\n",
      "Step: 627 | train loss: 0.000002 | test accuracy: 0.923355\n",
      "Step: 628 | train loss: 0.000002 | test accuracy: 0.923410\n",
      "Step: 629 | train loss: 0.000001 | test accuracy: 0.923466\n",
      "Step: 630 | train loss: 0.000002 | test accuracy: 0.923521\n",
      "Step: 631 | train loss: 0.000001 | test accuracy: 0.923576\n",
      "Step: 632 | train loss: 0.000001 | test accuracy: 0.923631\n",
      "Step: 633 | train loss: 0.000002 | test accuracy: 0.923686\n",
      "Step: 634 | train loss: 0.000002 | test accuracy: 0.923740\n",
      "Step: 635 | train loss: 0.000001 | test accuracy: 0.923795\n",
      "Step: 636 | train loss: 0.000002 | test accuracy: 0.923849\n",
      "Step: 637 | train loss: 0.000002 | test accuracy: 0.923903\n",
      "Step: 638 | train loss: 0.000001 | test accuracy: 0.923957\n",
      "Step: 639 | train loss: 0.000001 | test accuracy: 0.924010\n",
      "Step: 640 | train loss: 0.000002 | test accuracy: 0.924064\n",
      "Step: 641 | train loss: 0.000001 | test accuracy: 0.924117\n",
      "Step: 642 | train loss: 0.000001 | test accuracy: 0.924171\n",
      "Step: 643 | train loss: 0.000001 | test accuracy: 0.924224\n",
      "Step: 644 | train loss: 0.000002 | test accuracy: 0.924276\n",
      "Step: 645 | train loss: 0.000002 | test accuracy: 0.924329\n",
      "Step: 646 | train loss: 0.000002 | test accuracy: 0.924382\n",
      "Step: 647 | train loss: 0.000003 | test accuracy: 0.924434\n",
      "Step: 648 | train loss: 0.000001 | test accuracy: 0.924486\n",
      "Step: 649 | train loss: 0.000001 | test accuracy: 0.924538\n",
      "Step: 650 | train loss: 0.000002 | test accuracy: 0.924590\n",
      "Step: 651 | train loss: 0.000002 | test accuracy: 0.924642\n",
      "Step: 652 | train loss: 0.000001 | test accuracy: 0.924694\n",
      "Step: 653 | train loss: 0.000002 | test accuracy: 0.924745\n",
      "Step: 654 | train loss: 0.000002 | test accuracy: 0.924796\n",
      "Step: 655 | train loss: 0.000002 | test accuracy: 0.924848\n",
      "Step: 656 | train loss: 0.000001 | test accuracy: 0.924899\n",
      "Step: 657 | train loss: 0.000001 | test accuracy: 0.924949\n",
      "Step: 658 | train loss: 0.000001 | test accuracy: 0.925000\n",
      "Step: 659 | train loss: 0.000001 | test accuracy: 0.925050\n",
      "Step: 660 | train loss: 0.000002 | test accuracy: 0.925101\n",
      "Step: 661 | train loss: 0.000002 | test accuracy: 0.925151\n",
      "Step: 662 | train loss: 0.000002 | test accuracy: 0.925201\n",
      "Step: 663 | train loss: 0.000002 | test accuracy: 0.925251\n",
      "Step: 664 | train loss: 0.000002 | test accuracy: 0.925301\n",
      "Step: 665 | train loss: 0.000002 | test accuracy: 0.925350\n",
      "Step: 666 | train loss: 0.000001 | test accuracy: 0.925400\n",
      "Step: 667 | train loss: 0.000002 | test accuracy: 0.925449\n",
      "Step: 668 | train loss: 0.000002 | test accuracy: 0.925498\n",
      "Step: 669 | train loss: 0.000002 | test accuracy: 0.925547\n",
      "Step: 670 | train loss: 0.000000 | test accuracy: 0.925596\n",
      "Step: 671 | train loss: 0.000002 | test accuracy: 0.925645\n",
      "Step: 672 | train loss: 0.000002 | test accuracy: 0.925693\n",
      "Step: 673 | train loss: 0.000002 | test accuracy: 0.925742\n",
      "Step: 674 | train loss: 0.000002 | test accuracy: 0.925790\n",
      "Step: 675 | train loss: 0.000002 | test accuracy: 0.925838\n",
      "Step: 676 | train loss: 0.000002 | test accuracy: 0.925886\n",
      "Step: 677 | train loss: 0.000002 | test accuracy: 0.925934\n",
      "Step: 678 | train loss: 0.000001 | test accuracy: 0.925982\n",
      "Step: 679 | train loss: 0.000002 | test accuracy: 0.926029\n",
      "Step: 680 | train loss: 0.000001 | test accuracy: 0.926077\n",
      "Step: 681 | train loss: 0.000001 | test accuracy: 0.926124\n",
      "Step: 682 | train loss: 0.000002 | test accuracy: 0.926171\n",
      "Step: 683 | train loss: 0.000001 | test accuracy: 0.926218\n",
      "Step: 684 | train loss: 0.000001 | test accuracy: 0.926265\n",
      "Step: 685 | train loss: 0.000002 | test accuracy: 0.926312\n",
      "Step: 686 | train loss: 0.000002 | test accuracy: 0.926359\n",
      "Step: 687 | train loss: 0.000002 | test accuracy: 0.926405\n",
      "Step: 688 | train loss: 0.000002 | test accuracy: 0.926451\n",
      "Step: 689 | train loss: 0.000001 | test accuracy: 0.926498\n",
      "Step: 690 | train loss: 0.000001 | test accuracy: 0.926544\n",
      "Step: 691 | train loss: 0.000001 | test accuracy: 0.926590\n",
      "Step: 692 | train loss: 0.000002 | test accuracy: 0.926635\n",
      "Step: 693 | train loss: 0.000001 | test accuracy: 0.926681\n",
      "Step: 694 | train loss: 0.000002 | test accuracy: 0.926727\n",
      "Step: 695 | train loss: 0.000001 | test accuracy: 0.926772\n",
      "Step: 696 | train loss: 0.000001 | test accuracy: 0.926817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 697 | train loss: 0.000001 | test accuracy: 0.926862\n",
      "Step: 698 | train loss: 0.000001 | test accuracy: 0.926907\n",
      "Step: 699 | train loss: 0.000002 | test accuracy: 0.926952\n",
      "Step: 700 | train loss: 0.000001 | test accuracy: 0.926997\n",
      "Step: 701 | train loss: 0.000002 | test accuracy: 0.927042\n",
      "Step: 702 | train loss: 0.000001 | test accuracy: 0.927086\n",
      "Step: 703 | train loss: 0.000002 | test accuracy: 0.927131\n",
      "Step: 704 | train loss: 0.000002 | test accuracy: 0.927175\n",
      "Step: 705 | train loss: 0.000001 | test accuracy: 0.927219\n",
      "Step: 706 | train loss: 0.000002 | test accuracy: 0.927263\n",
      "Step: 707 | train loss: 0.000001 | test accuracy: 0.927307\n",
      "Step: 708 | train loss: 0.000001 | test accuracy: 0.927351\n",
      "Step: 709 | train loss: 0.000002 | test accuracy: 0.927394\n",
      "Step: 710 | train loss: 0.000001 | test accuracy: 0.927438\n",
      "Step: 711 | train loss: 0.000001 | test accuracy: 0.927481\n",
      "Step: 712 | train loss: 0.000001 | test accuracy: 0.927525\n",
      "Step: 713 | train loss: 0.000001 | test accuracy: 0.927568\n",
      "Step: 714 | train loss: 0.000001 | test accuracy: 0.927611\n",
      "Step: 715 | train loss: 0.000001 | test accuracy: 0.927654\n",
      "Step: 716 | train loss: 0.000001 | test accuracy: 0.927696\n",
      "Step: 717 | train loss: 0.000001 | test accuracy: 0.927739\n",
      "Step: 718 | train loss: 0.000001 | test accuracy: 0.927782\n",
      "Step: 719 | train loss: 0.000001 | test accuracy: 0.927824\n",
      "Step: 720 | train loss: 0.000002 | test accuracy: 0.927866\n",
      "Step: 721 | train loss: 0.000001 | test accuracy: 0.927909\n",
      "Step: 722 | train loss: 0.000000 | test accuracy: 0.927951\n",
      "Step: 723 | train loss: 0.000002 | test accuracy: 0.927993\n",
      "Step: 724 | train loss: 0.000001 | test accuracy: 0.928034\n",
      "Step: 725 | train loss: 0.000001 | test accuracy: 0.928076\n",
      "Step: 726 | train loss: 0.000002 | test accuracy: 0.928118\n",
      "Step: 727 | train loss: 0.000001 | test accuracy: 0.928159\n",
      "Step: 728 | train loss: 0.000001 | test accuracy: 0.928201\n",
      "Step: 729 | train loss: 0.000001 | test accuracy: 0.928242\n",
      "Step: 730 | train loss: 0.000001 | test accuracy: 0.928283\n",
      "Step: 731 | train loss: 0.000001 | test accuracy: 0.928324\n",
      "Step: 732 | train loss: 0.000002 | test accuracy: 0.928365\n",
      "Step: 733 | train loss: 0.000002 | test accuracy: 0.928406\n",
      "Step: 734 | train loss: 0.000002 | test accuracy: 0.928447\n",
      "Step: 735 | train loss: 0.000001 | test accuracy: 0.928487\n",
      "Step: 736 | train loss: 0.000002 | test accuracy: 0.928528\n",
      "Step: 737 | train loss: 0.000002 | test accuracy: 0.928568\n",
      "Step: 738 | train loss: 0.000001 | test accuracy: 0.928608\n",
      "Step: 739 | train loss: 0.000001 | test accuracy: 0.928649\n",
      "Step: 740 | train loss: 0.000002 | test accuracy: 0.928689\n",
      "Step: 741 | train loss: 0.000002 | test accuracy: 0.928729\n",
      "Step: 742 | train loss: 0.000001 | test accuracy: 0.928769\n",
      "Step: 743 | train loss: 0.000001 | test accuracy: 0.928808\n",
      "Step: 744 | train loss: 0.000001 | test accuracy: 0.928848\n",
      "Step: 745 | train loss: 0.000001 | test accuracy: 0.928887\n",
      "Step: 746 | train loss: 0.000002 | test accuracy: 0.928927\n",
      "Step: 747 | train loss: 0.000002 | test accuracy: 0.928966\n",
      "Step: 748 | train loss: 0.000001 | test accuracy: 0.929005\n",
      "Step: 749 | train loss: 0.000001 | test accuracy: 0.929044\n",
      "Step: 750 | train loss: 0.000002 | test accuracy: 0.929083\n",
      "Step: 751 | train loss: 0.000001 | test accuracy: 0.929122\n",
      "Step: 752 | train loss: 0.000001 | test accuracy: 0.929161\n",
      "Step: 753 | train loss: 0.000002 | test accuracy: 0.929200\n",
      "Step: 754 | train loss: 0.000003 | test accuracy: 0.929238\n",
      "Step: 755 | train loss: 0.000001 | test accuracy: 0.929277\n",
      "Step: 756 | train loss: 0.000002 | test accuracy: 0.929315\n",
      "Step: 757 | train loss: 0.000001 | test accuracy: 0.929354\n",
      "Step: 758 | train loss: 0.000002 | test accuracy: 0.929392\n",
      "Step: 759 | train loss: 0.000002 | test accuracy: 0.929430\n",
      "Step: 760 | train loss: 0.000001 | test accuracy: 0.929468\n",
      "Step: 761 | train loss: 0.000002 | test accuracy: 0.929506\n",
      "Step: 762 | train loss: 0.000001 | test accuracy: 0.929543\n",
      "Step: 763 | train loss: 0.000002 | test accuracy: 0.929581\n",
      "Step: 764 | train loss: 0.000001 | test accuracy: 0.929619\n",
      "Step: 765 | train loss: 0.000001 | test accuracy: 0.929656\n",
      "Step: 766 | train loss: 0.000001 | test accuracy: 0.929694\n",
      "Step: 767 | train loss: 0.000001 | test accuracy: 0.929731\n",
      "Step: 768 | train loss: 0.000002 | test accuracy: 0.929768\n",
      "Step: 769 | train loss: 0.000001 | test accuracy: 0.929805\n",
      "Step: 770 | train loss: 0.000001 | test accuracy: 0.929842\n",
      "Step: 771 | train loss: 0.000001 | test accuracy: 0.929879\n",
      "Step: 772 | train loss: 0.000002 | test accuracy: 0.929916\n",
      "Step: 773 | train loss: 0.000001 | test accuracy: 0.929953\n",
      "Step: 774 | train loss: 0.000001 | test accuracy: 0.929989\n",
      "Step: 775 | train loss: 0.000002 | test accuracy: 0.930026\n",
      "Step: 776 | train loss: 0.000001 | test accuracy: 0.930062\n",
      "Step: 777 | train loss: 0.000001 | test accuracy: 0.930099\n",
      "Step: 778 | train loss: 0.000001 | test accuracy: 0.930135\n",
      "Step: 779 | train loss: 0.000001 | test accuracy: 0.930171\n",
      "Step: 780 | train loss: 0.000002 | test accuracy: 0.930207\n",
      "Step: 781 | train loss: 0.000002 | test accuracy: 0.930243\n",
      "Step: 782 | train loss: 0.000002 | test accuracy: 0.930279\n",
      "Step: 783 | train loss: 0.000001 | test accuracy: 0.930315\n",
      "Step: 784 | train loss: 0.000001 | test accuracy: 0.930350\n",
      "Step: 785 | train loss: 0.000001 | test accuracy: 0.930386\n",
      "Step: 786 | train loss: 0.000000 | test accuracy: 0.930421\n",
      "Step: 787 | train loss: 0.000001 | test accuracy: 0.930457\n",
      "Step: 788 | train loss: 0.000001 | test accuracy: 0.930492\n",
      "Step: 789 | train loss: 0.000001 | test accuracy: 0.930527\n",
      "Step: 790 | train loss: 0.000001 | test accuracy: 0.930563\n",
      "Step: 791 | train loss: 0.000002 | test accuracy: 0.930598\n",
      "Step: 792 | train loss: 0.000001 | test accuracy: 0.930633\n",
      "Step: 793 | train loss: 0.000001 | test accuracy: 0.930668\n",
      "Step: 794 | train loss: 0.000002 | test accuracy: 0.930702\n",
      "Step: 795 | train loss: 0.000001 | test accuracy: 0.930737\n",
      "Step: 796 | train loss: 0.000002 | test accuracy: 0.930772\n",
      "Step: 797 | train loss: 0.000001 | test accuracy: 0.930806\n",
      "Step: 798 | train loss: 0.000001 | test accuracy: 0.930841\n",
      "Step: 799 | train loss: 0.000001 | test accuracy: 0.930875\n",
      "Step: 800 | train loss: 0.000001 | test accuracy: 0.930909\n",
      "Step: 801 | train loss: 0.000000 | test accuracy: 0.930943\n",
      "Step: 802 | train loss: 0.000001 | test accuracy: 0.930978\n",
      "Step: 803 | train loss: 0.000002 | test accuracy: 0.931012\n",
      "Step: 804 | train loss: 0.000001 | test accuracy: 0.931046\n",
      "Step: 805 | train loss: 0.000001 | test accuracy: 0.931079\n",
      "Step: 806 | train loss: 0.000001 | test accuracy: 0.931113\n",
      "Step: 807 | train loss: 0.000001 | test accuracy: 0.931147\n",
      "Step: 808 | train loss: 0.000001 | test accuracy: 0.931180\n",
      "Step: 809 | train loss: 0.000001 | test accuracy: 0.931214\n",
      "Step: 810 | train loss: 0.000002 | test accuracy: 0.931247\n",
      "Step: 811 | train loss: 0.000001 | test accuracy: 0.931281\n",
      "Step: 812 | train loss: 0.000001 | test accuracy: 0.931314\n",
      "Step: 813 | train loss: 0.000001 | test accuracy: 0.931347\n",
      "Step: 814 | train loss: 0.000001 | test accuracy: 0.931380\n",
      "Step: 815 | train loss: 0.000001 | test accuracy: 0.931413\n",
      "Step: 816 | train loss: 0.000001 | test accuracy: 0.931446\n",
      "Step: 817 | train loss: 0.000001 | test accuracy: 0.931479\n",
      "Step: 818 | train loss: 0.000002 | test accuracy: 0.931512\n",
      "Step: 819 | train loss: 0.000001 | test accuracy: 0.931545\n",
      "Step: 820 | train loss: 0.000001 | test accuracy: 0.931577\n",
      "Step: 821 | train loss: 0.000001 | test accuracy: 0.931610\n",
      "Step: 822 | train loss: 0.000001 | test accuracy: 0.931642\n",
      "Step: 823 | train loss: 0.000001 | test accuracy: 0.931675\n",
      "Step: 824 | train loss: 0.000002 | test accuracy: 0.931707\n",
      "Step: 825 | train loss: 0.000001 | test accuracy: 0.931739\n",
      "Step: 826 | train loss: 0.000002 | test accuracy: 0.931771\n",
      "Step: 827 | train loss: 0.000001 | test accuracy: 0.931804\n",
      "Step: 828 | train loss: 0.000001 | test accuracy: 0.931836\n",
      "Step: 829 | train loss: 0.000001 | test accuracy: 0.931867\n",
      "Step: 830 | train loss: 0.000001 | test accuracy: 0.931899\n",
      "Step: 831 | train loss: 0.000001 | test accuracy: 0.931931\n",
      "Step: 832 | train loss: 0.000001 | test accuracy: 0.931963\n",
      "Step: 833 | train loss: 0.000001 | test accuracy: 0.931994\n",
      "Step: 834 | train loss: 0.000002 | test accuracy: 0.932026\n",
      "Step: 835 | train loss: 0.000001 | test accuracy: 0.932057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 836 | train loss: 0.000002 | test accuracy: 0.932089\n",
      "Step: 837 | train loss: 0.000001 | test accuracy: 0.932120\n",
      "Step: 838 | train loss: 0.000001 | test accuracy: 0.932151\n",
      "Step: 839 | train loss: 0.000001 | test accuracy: 0.932183\n",
      "Step: 840 | train loss: 0.000001 | test accuracy: 0.932214\n",
      "Step: 841 | train loss: 0.000001 | test accuracy: 0.932245\n",
      "Step: 842 | train loss: 0.000001 | test accuracy: 0.932276\n",
      "Step: 843 | train loss: 0.000001 | test accuracy: 0.932306\n",
      "Step: 844 | train loss: 0.000001 | test accuracy: 0.932337\n",
      "Step: 845 | train loss: 0.000001 | test accuracy: 0.932368\n",
      "Step: 846 | train loss: 0.000001 | test accuracy: 0.932399\n",
      "Step: 847 | train loss: 0.000001 | test accuracy: 0.932429\n",
      "Step: 848 | train loss: 0.000001 | test accuracy: 0.932460\n",
      "Step: 849 | train loss: 0.000002 | test accuracy: 0.932490\n",
      "Step: 850 | train loss: 0.000001 | test accuracy: 0.932521\n",
      "Step: 851 | train loss: 0.000001 | test accuracy: 0.932551\n",
      "Step: 852 | train loss: 0.000002 | test accuracy: 0.932581\n",
      "Step: 853 | train loss: 0.000001 | test accuracy: 0.932611\n",
      "Step: 854 | train loss: 0.000001 | test accuracy: 0.932641\n",
      "Step: 855 | train loss: 0.000001 | test accuracy: 0.932671\n",
      "Step: 856 | train loss: 0.000001 | test accuracy: 0.932701\n",
      "Step: 857 | train loss: 0.000001 | test accuracy: 0.932731\n",
      "Step: 858 | train loss: 0.000001 | test accuracy: 0.932761\n",
      "Step: 859 | train loss: 0.000001 | test accuracy: 0.932791\n",
      "Step: 860 | train loss: 0.000001 | test accuracy: 0.932820\n",
      "Step: 861 | train loss: 0.000001 | test accuracy: 0.932850\n",
      "Step: 862 | train loss: 0.000001 | test accuracy: 0.932880\n",
      "Step: 863 | train loss: 0.000001 | test accuracy: 0.932909\n",
      "Step: 864 | train loss: 0.000001 | test accuracy: 0.932938\n",
      "Step: 865 | train loss: 0.000001 | test accuracy: 0.932968\n",
      "Step: 866 | train loss: 0.000001 | test accuracy: 0.932997\n",
      "Step: 867 | train loss: 0.000001 | test accuracy: 0.933026\n",
      "Step: 868 | train loss: 0.000002 | test accuracy: 0.933055\n",
      "Step: 869 | train loss: 0.000002 | test accuracy: 0.933084\n",
      "Step: 870 | train loss: 0.000001 | test accuracy: 0.933113\n",
      "Step: 871 | train loss: 0.000001 | test accuracy: 0.933142\n",
      "Step: 872 | train loss: 0.000001 | test accuracy: 0.933171\n",
      "Step: 873 | train loss: 0.000001 | test accuracy: 0.933200\n",
      "Step: 874 | train loss: 0.000001 | test accuracy: 0.933229\n",
      "Step: 875 | train loss: 0.000002 | test accuracy: 0.933257\n",
      "Step: 876 | train loss: 0.000001 | test accuracy: 0.933286\n",
      "Step: 877 | train loss: 0.000002 | test accuracy: 0.933314\n",
      "Step: 878 | train loss: 0.000001 | test accuracy: 0.933343\n",
      "Step: 879 | train loss: 0.000001 | test accuracy: 0.933371\n",
      "Step: 880 | train loss: 0.000001 | test accuracy: 0.933400\n",
      "Step: 881 | train loss: 0.000001 | test accuracy: 0.933428\n",
      "Step: 882 | train loss: 0.000001 | test accuracy: 0.933456\n",
      "Step: 883 | train loss: 0.000001 | test accuracy: 0.933484\n",
      "Step: 884 | train loss: 0.000002 | test accuracy: 0.933512\n",
      "Step: 885 | train loss: 0.000001 | test accuracy: 0.933540\n",
      "Step: 886 | train loss: 0.000002 | test accuracy: 0.933568\n",
      "Step: 887 | train loss: 0.000001 | test accuracy: 0.933596\n",
      "Step: 888 | train loss: 0.000001 | test accuracy: 0.933624\n",
      "Step: 889 | train loss: 0.000001 | test accuracy: 0.933652\n",
      "Step: 890 | train loss: 0.000001 | test accuracy: 0.933679\n",
      "Step: 891 | train loss: 0.000002 | test accuracy: 0.933707\n",
      "Step: 892 | train loss: 0.000001 | test accuracy: 0.933735\n",
      "Step: 893 | train loss: 0.000001 | test accuracy: 0.933762\n",
      "Step: 894 | train loss: 0.000002 | test accuracy: 0.933790\n",
      "Step: 895 | train loss: 0.000002 | test accuracy: 0.933817\n",
      "Step: 896 | train loss: 0.000001 | test accuracy: 0.933844\n",
      "Step: 897 | train loss: 0.000001 | test accuracy: 0.933872\n",
      "Step: 898 | train loss: 0.000001 | test accuracy: 0.933899\n",
      "Step: 899 | train loss: 0.000001 | test accuracy: 0.933926\n",
      "Step: 900 | train loss: 0.000001 | test accuracy: 0.933953\n",
      "Step: 901 | train loss: 0.000001 | test accuracy: 0.933980\n",
      "Step: 902 | train loss: 0.000001 | test accuracy: 0.934007\n",
      "Step: 903 | train loss: 0.000001 | test accuracy: 0.934034\n",
      "Step: 904 | train loss: 0.000000 | test accuracy: 0.934061\n",
      "Step: 905 | train loss: 0.000001 | test accuracy: 0.934088\n",
      "Step: 906 | train loss: 0.000001 | test accuracy: 0.934114\n",
      "Step: 907 | train loss: 0.000001 | test accuracy: 0.934141\n",
      "Step: 908 | train loss: 0.000002 | test accuracy: 0.934168\n",
      "Step: 909 | train loss: 0.000002 | test accuracy: 0.934194\n",
      "Step: 910 | train loss: 0.000002 | test accuracy: 0.934221\n",
      "Step: 911 | train loss: 0.000001 | test accuracy: 0.934247\n",
      "Step: 912 | train loss: 0.000001 | test accuracy: 0.934273\n",
      "Step: 913 | train loss: 0.000001 | test accuracy: 0.934300\n",
      "Step: 914 | train loss: 0.000001 | test accuracy: 0.934326\n",
      "Step: 915 | train loss: 0.000000 | test accuracy: 0.934352\n",
      "Step: 916 | train loss: 0.000001 | test accuracy: 0.934378\n",
      "Step: 917 | train loss: 0.000002 | test accuracy: 0.934404\n",
      "Step: 918 | train loss: 0.000000 | test accuracy: 0.934431\n",
      "Step: 919 | train loss: 0.000001 | test accuracy: 0.934457\n",
      "Step: 920 | train loss: 0.000001 | test accuracy: 0.934482\n",
      "Step: 921 | train loss: 0.000001 | test accuracy: 0.934508\n",
      "Step: 922 | train loss: 0.000001 | test accuracy: 0.934534\n",
      "Step: 923 | train loss: 0.000001 | test accuracy: 0.934560\n",
      "Step: 924 | train loss: 0.000001 | test accuracy: 0.934586\n",
      "Step: 925 | train loss: 0.000001 | test accuracy: 0.934611\n",
      "Step: 926 | train loss: 0.000001 | test accuracy: 0.934637\n",
      "Step: 927 | train loss: 0.000001 | test accuracy: 0.934662\n",
      "Step: 928 | train loss: 0.000001 | test accuracy: 0.934688\n",
      "Step: 929 | train loss: 0.000001 | test accuracy: 0.934713\n",
      "Step: 930 | train loss: 0.000001 | test accuracy: 0.934739\n",
      "Step: 931 | train loss: 0.000001 | test accuracy: 0.934764\n",
      "Step: 932 | train loss: 0.000001 | test accuracy: 0.934789\n",
      "Step: 933 | train loss: 0.000001 | test accuracy: 0.934814\n",
      "Step: 934 | train loss: 0.000001 | test accuracy: 0.934840\n",
      "Step: 935 | train loss: 0.000001 | test accuracy: 0.934865\n",
      "Step: 936 | train loss: 0.000001 | test accuracy: 0.934890\n",
      "Step: 937 | train loss: 0.000001 | test accuracy: 0.934915\n",
      "Step: 938 | train loss: 0.000001 | test accuracy: 0.934940\n",
      "Step: 939 | train loss: 0.000001 | test accuracy: 0.934965\n",
      "Step: 940 | train loss: 0.000001 | test accuracy: 0.934989\n",
      "Step: 941 | train loss: 0.000000 | test accuracy: 0.935014\n",
      "Step: 942 | train loss: 0.000001 | test accuracy: 0.935039\n",
      "Step: 943 | train loss: 0.000001 | test accuracy: 0.935064\n",
      "Step: 944 | train loss: 0.000002 | test accuracy: 0.935088\n",
      "Step: 945 | train loss: 0.000001 | test accuracy: 0.935113\n",
      "Step: 946 | train loss: 0.000001 | test accuracy: 0.935137\n",
      "Step: 947 | train loss: 0.000001 | test accuracy: 0.935162\n",
      "Step: 948 | train loss: 0.000001 | test accuracy: 0.935186\n",
      "Step: 949 | train loss: 0.000001 | test accuracy: 0.935211\n",
      "Step: 950 | train loss: 0.000001 | test accuracy: 0.935235\n",
      "Step: 951 | train loss: 0.000001 | test accuracy: 0.935259\n",
      "Step: 952 | train loss: 0.000001 | test accuracy: 0.935283\n",
      "Step: 953 | train loss: 0.000001 | test accuracy: 0.935308\n",
      "Step: 954 | train loss: 0.000001 | test accuracy: 0.935332\n",
      "Step: 955 | train loss: 0.000001 | test accuracy: 0.935356\n",
      "Step: 956 | train loss: 0.000000 | test accuracy: 0.935380\n",
      "Step: 957 | train loss: 0.000001 | test accuracy: 0.935404\n",
      "Step: 958 | train loss: 0.000001 | test accuracy: 0.935428\n",
      "Step: 959 | train loss: 0.000001 | test accuracy: 0.935451\n",
      "Step: 960 | train loss: 0.000001 | test accuracy: 0.935475\n",
      "Step: 961 | train loss: 0.000001 | test accuracy: 0.935499\n",
      "Step: 962 | train loss: 0.000001 | test accuracy: 0.935523\n",
      "Step: 963 | train loss: 0.000001 | test accuracy: 0.935546\n",
      "Step: 964 | train loss: 0.000001 | test accuracy: 0.935570\n",
      "Step: 965 | train loss: 0.000001 | test accuracy: 0.935593\n",
      "Step: 966 | train loss: 0.000001 | test accuracy: 0.935617\n",
      "Step: 967 | train loss: 0.000001 | test accuracy: 0.935641\n",
      "Step: 968 | train loss: 0.000002 | test accuracy: 0.935664\n",
      "Step: 969 | train loss: 0.000001 | test accuracy: 0.935687\n",
      "Step: 970 | train loss: 0.000001 | test accuracy: 0.935711\n",
      "Step: 971 | train loss: 0.000001 | test accuracy: 0.935734\n",
      "Step: 972 | train loss: 0.000001 | test accuracy: 0.935757\n",
      "Step: 973 | train loss: 0.000001 | test accuracy: 0.935780\n",
      "Step: 974 | train loss: 0.000001 | test accuracy: 0.935803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 975 | train loss: 0.000001 | test accuracy: 0.935826\n",
      "Step: 976 | train loss: 0.000001 | test accuracy: 0.935850\n",
      "Step: 977 | train loss: 0.000001 | test accuracy: 0.935873\n",
      "Step: 978 | train loss: 0.000001 | test accuracy: 0.935895\n",
      "Step: 979 | train loss: 0.000001 | test accuracy: 0.935918\n",
      "Step: 980 | train loss: 0.000000 | test accuracy: 0.935941\n",
      "Step: 981 | train loss: 0.000001 | test accuracy: 0.935964\n",
      "Step: 982 | train loss: 0.000002 | test accuracy: 0.935987\n",
      "Step: 983 | train loss: 0.000001 | test accuracy: 0.936009\n",
      "Step: 984 | train loss: 0.000001 | test accuracy: 0.936032\n",
      "Step: 985 | train loss: 0.000001 | test accuracy: 0.936055\n",
      "Step: 986 | train loss: 0.000001 | test accuracy: 0.936077\n",
      "Step: 987 | train loss: 0.000001 | test accuracy: 0.936100\n",
      "Step: 988 | train loss: 0.000001 | test accuracy: 0.936122\n",
      "Step: 989 | train loss: 0.000001 | test accuracy: 0.936145\n",
      "Step: 990 | train loss: 0.000001 | test accuracy: 0.936167\n",
      "Step: 991 | train loss: 0.000001 | test accuracy: 0.936190\n",
      "Step: 992 | train loss: 0.000001 | test accuracy: 0.936212\n",
      "Step: 993 | train loss: 0.000001 | test accuracy: 0.936234\n",
      "Step: 994 | train loss: 0.000001 | test accuracy: 0.936256\n",
      "Step: 995 | train loss: 0.000001 | test accuracy: 0.936278\n",
      "Step: 996 | train loss: 0.000001 | test accuracy: 0.936301\n",
      "Step: 997 | train loss: 0.000001 | test accuracy: 0.936323\n",
      "Step: 998 | train loss: 0.000001 | test accuracy: 0.936345\n",
      "Step: 999 | train loss: 0.000001 | test accuracy: 0.936367\n",
      "Step: 1000 | train loss: 0.000001 | test accuracy: 0.936389\n"
     ]
    }
   ],
   "source": [
    "#define network_input\n",
    "\n",
    "with tf.variable_scope('Inputs'):\n",
    "    tf_x = tf.placeholder(tf.float32, [None, 112*92]) / 255.\n",
    "    image = tf.reshape(tf_x, [-1, 112, 92, 1])              # (batch, height, width, channel)\n",
    "    tf_y = tf.placeholder(tf.int32, [None, 40])            # input y\n",
    "\n",
    "# CNN No.1\n",
    "# with tf.variable_scope('Net'):\n",
    "#     conv1 = tf.layers.conv2d(   # shape (112, 92, 1)\n",
    "#         inputs=image,\n",
    "#         filters=16,\n",
    "#         kernel_size=4,\n",
    "#         strides=1,\n",
    "#         padding='same',\n",
    "#         activation=tf.nn.relu,\n",
    "#         name='conv_1'\n",
    "#     )           # -> (112, 92, 16)\n",
    "#     pool1 = tf.layers.max_pooling2d(\n",
    "#         conv1,\n",
    "#         pool_size=2,\n",
    "#         strides=2,\n",
    "#         name='pool1'\n",
    "#     )           # -> (56, 46, 16)\n",
    "#     conv2 = tf.layers.conv2d(pool1, 16, 2, 1, 'same', activation=tf.nn.relu, name='conv2')    # -> (56, 46, 16)\n",
    "#     pool2 = tf.layers.max_pooling2d(conv2, 2, 2, name='pool2')    # -> (28, 23, 16)\n",
    "#     flat = tf.reshape(pool2, [-1, 28*23*16], name='flatten')          # -> (28*23*16, )\n",
    "#     output = tf.layers.dense(flat, 40,name='fully_connected')              # output layer\n",
    "\n",
    "\n",
    "with tf.variable_scope('Net'):\n",
    "    conv1 = tf.layers.conv2d(   # shape (112, 92, 1)\n",
    "        inputs=image,\n",
    "        filters=20,\n",
    "        kernel_size=4,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        activation=tf.nn.relu,\n",
    "        name='conv_1'\n",
    "    )           # -> (112, 92, 20)\n",
    "    pool1 = tf.layers.max_pooling2d(\n",
    "        conv1,\n",
    "        pool_size=2,\n",
    "        strides=2,\n",
    "        name='pool1'\n",
    "    )           # -> (56, 46, 20)\n",
    "    conv2 = tf.layers.conv2d(pool1, 40, 3, 1, 'same', activation=tf.nn.relu, name='conv2')    # -> (56, 46, 40)\n",
    "    pool2 = tf.layers.max_pooling2d(conv2, 2, 2, name='pool2')    # -> (28, 23, 40)\n",
    "    conv3 = tf.layers.conv2d(pool2, 60, 2, 1, 'same', activation=tf.nn.relu, name='conv3')    # -> (28, 23, 60)\n",
    "    pool3 = tf.layers.max_pooling2d(conv3, 2, 2, name='pool3')    # -> (14, 12, 60)\n",
    "    flat = tf.reshape(pool3, [-1, 14*11*60], name='flatten')          # -> (14*11*60, )\n",
    "    output = tf.layers.dense(flat, 40,name='fully_connected')              # output layer\n",
    "    \n",
    "# global_step = tf.Variable(0, trainable=False)\n",
    "# starter_learning_rate = 0.0001\n",
    "# learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,100000, 0.96, staircase=True)\n",
    "\n",
    "# loss = tf.losses.softmax_cross_entropy(onehot_labels=tf_y, logits=output)           # compute cost\n",
    "# train_op = tf.train.AdamOptimizer(LR).minimize(loss)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "#0.002\n",
    "starter_learning_rate = 0.01\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,100000, 0.96, staircase=True)\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(onehot_labels=tf_y, logits=output)   # compute cost\n",
    "\n",
    "accuracy = tf.metrics.accuracy(          # return (acc, update_op), and create 2 local variables\n",
    "    labels=tf.argmax(tf_y, axis=1), predictions=tf.argmax(output, axis=1),)[1]\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "tf.summary.scalar('loss', loss)     # add loss to scalar summary\n",
    "tf.summary.scalar('acc', accuracy)\n",
    "\n",
    "sess = tf.Session()\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()) # the local var is for accuracy_op\n",
    "sess.run(init_op)     # initialize var in graph\n",
    "\n",
    "# following function (plot_with_labels) is for visualization, can be ignored if not interested\n",
    "from matplotlib import cm\n",
    "try: from sklearn.manifold import TSNE; HAS_SK = True\n",
    "except: HAS_SK = False; print('\\nPlease install sklearn for layer visualization\\n')\n",
    "def plot_with_labels(lowDWeights, labels):\n",
    "    plt.cla(); X, Y = lowDWeights[:, 0], lowDWeights[:, 1]\n",
    "    for x, y, s in zip(X, Y, labels):\n",
    "        c = cm.rainbow(int(255 * s / 40)); plt.text(x, y, s, backgroundcolor=c, fontsize=9)\n",
    "    plt.xlim(X.min(), X.max()); plt.ylim(Y.min(), Y.max()); plt.title('Visualize last layer'); plt.show(); plt.pause(0.01)\n",
    "\n",
    "plt.ion()\n",
    "for step in range(1001):\n",
    "    b_x, b_y = get_batch(BATCH_SIZE)\n",
    "    _, loss_ = sess.run([train_op, loss], {tf_x: b_x, tf_y: b_y})\n",
    "    if step % 50 == 0:\n",
    "        accuracy_, flat_representation = sess.run([accuracy, flat], {tf_x: X_test, tf_y: Y_test})\n",
    "        print('Step:', step, '| train loss: %f' % loss_, '| test accuracy: %f' % accuracy_)\n",
    "\n",
    "#         if HAS_SK:\n",
    "#             # Visualization of trained flatten layer (T-SNE)\n",
    "#             tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000); plot_only = Y_test.shape[0]\n",
    "#             low_dim_embs = tsne.fit_transform(flat_representation[:plot_only, :])\n",
    "#             labels = np.argmax(Y_test, axis=1)[:plot_only]; plot_with_labels(low_dim_embs, labels)\n",
    "    else:\n",
    "        accuracy_, flat_representation = sess.run([accuracy, flat], {tf_x: X_test, tf_y: Y_test})\n",
    "        print('Step:', step, '| train loss: %f' % loss_, '| test accuracy: %f' % accuracy_)\n",
    "plt.ioff()\n",
    "\n",
    "# # print 10 predictions from test data\n",
    "# test_output = sess.run(output, {tf_x: test_x[:10]})\n",
    "# pred_y = np.argmax(test_output, 1)\n",
    "# print(pred_y, 'prediction number')\n",
    "# print(np.argmax(test_y[:10], 1), 'real number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T07:39:03.009186Z",
     "start_time": "2019-01-30T07:39:00.094521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000 | train loss: 0.000001 | test accuracy: 0.936432\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12203b7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_, flat_representation, output_y = sess.run([accuracy, flat,output], {tf_x: X_test, tf_y: Y_test})\n",
    "print('Step:', step, '| train loss: %f' % loss_, '| test accuracy: %f' % accuracy_)\n",
    "\n",
    "if HAS_SK:\n",
    "    # Visualization of trained flatten layer (T-SNE)\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000); plot_only = Y_test.shape[0]\n",
    "    low_dim_embs = tsne.fit_transform(flat_representation[:plot_only, :])\n",
    "    labels = np.argmax(Y_test, axis=1)[:plot_only]; plot_with_labels(low_dim_embs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T07:39:09.521819Z",
     "start_time": "2019-01-30T07:39:09.013285Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions=tf.argmax(output_y, axis=1)\n",
    "acc, pred, onehot_predictions = sess.run([accuracy, output,predictions], {tf_x: X_test, tf_y: Y_test})\n",
    "test_people = 0\n",
    "face = X_test[test_people,:]\n",
    "face_reshaped = np.reshape(face, [112, 92])\n",
    "plt.figure()\n",
    "plt.title('test_image')\n",
    "plt.imshow(face_reshaped,cmap='Greys_r')\n",
    "person = onehot_predictions[test_people]\n",
    "plt.figure()\n",
    "plt.suptitle('all_related_faces')\n",
    "for faceNumber in range(1,11):\n",
    "    path = './ORL/s%d_%d.bmp' % (person + 1, faceNumber)\n",
    "    oriImage = PIL.Image.open(path)\n",
    "    imageArray = np.array(oriImage)\n",
    "    plt.subplot(4,3,faceNumber)\n",
    "    plt.imshow(imageArray,cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
